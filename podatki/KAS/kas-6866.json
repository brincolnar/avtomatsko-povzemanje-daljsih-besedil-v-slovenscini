{"1": ["1. UVOD", "Umetno nevronsko omrežje (UNO) je pojem, s katerim se srečujemo vse pogosteje na različnih področjih tehnike [16]. Predstavlja orodje za reševanje realnih problemov, za katere klasične, analitične metode ne zadostujejo. V sistemih, pogojenih z dodatnima kriterijema varnosti in zanesljivosti delovanja, se uporablja kot dodaten vir informacij pri sprejemanju končnih odločitev. Sestavljeno je iz velikega števila nevronov, ki so med seboj povezani in vzporedno obdelujejo podatke glede na dinamično stanje nevronskega omrežja in glede na zunanje vhode. Ker se je z učenjem sposobno prilagoditi vhodnim informacijam in postavljenim zahtevam, ga uvrščamo med adaptivne sisteme [10]. Z učenjem sta povezani tudi lastnosti asociativnosti in posploševanja [3]. UNO je tudi robusten sistem, saj lahko nekaj nevronov (procesnih enot) odstranimo, pa bo nevronsko omrežje še vedno pravilno delovalo, le rezultati bodo nekoliko slabši. Lastnosti robustnosti, učenja, asociativnosti in posploševanja dajejo UNO večjo fleksibilnost.  V močnostni elektrotehniki se najpogosteje srečujemo z uporabo odprtozančnih umetnih nevronskih omrežij in njim pripadajočim algoritmom učenja z vzvratnim širjenjem pogreška [15]. Ta nevronska omrežja so po uspešnem učenju sposobna podati razumne odgovore na vhodne podatke, ki jih niso še nikoli videla, aproksimirati funkcije s končnim številom nezveznosti ter razvrščati vhodne vektorje na način, kot jim ga določimo. Vendar pa osnovna gradientna metoda algoritma učenja z vzvratnim širjenjem pogreška, t.j. metoda padajočih gradientov, v mnogih primerih ne konvergira k rešitvi zadosti hitro oz. je dostikrat sploh ne doseže. V namen pospeševanja in večje učinkovitosti pri učenju odprtozančnih omrežij so različni avtorji [16],[18] razvili izboljšane metode učenja z vzvratnim širjenjem pogreška, ki lahko konvergirajo tudi do stokrat hitreje. V drugem poglavju je opravljen pregled nad temi izboljšanimi metodami, njihova praktična uporaba v aplikacijah močnostne elektrotehnike pa je predstavljena v tretjem poglavju. V četrtem poglavju je podana primerjava teh metod na primeru razpoznave vklopnega toka transformatorja in razpoznave sinusnega toka, primerjava razpoznave vzorcev po naključno izbranih testnih vzorcih in povezava med testnim pogreškom in razpoznavanjem vzorca. Poglavje je zaključeno s prikazom odvisnosti učnega in testnega pogreška od števila učnih vzorcev. V petem poglavju so prikazani rezultati primerjave in predlogi uporabe metod.\n"], "2": ["2. NEVRONSKO OMREŽJE", ""], "3": ["2.1 Uvod", "Teoretični začetki nevronskih omrežij segajo v zgodnja 40. leta, ko sta avtorja McCulloch in Pitts [13] postavila matematični model živčne celice-nevrona. V naslednjih štiridesetih letih na področju nevronskih omrežij ni bilo bistvenega napredka. Med svetle točke spadata predstavitev učnega algoritma leta 1949, avtorja Hebba [7] in Widrowov adaptivni sistem na osnovi pragovnega elementa, ki mu je možno spreminjati uteži [3]. Ugotovili so, da je z njim mogoče reševati probleme, za katere deterministične rešitve sploh niso obstajale. Leta 1959 je bila postavljena zasnova perceptrona [19] in ustrezno pravilo učenja. Perceptron je eden prvih adaptivnih sistemov, saj je pozneje avtor uvedel tudi povratno zanko. Podobna zasnova je bila razvita leta 1961 [10], znana pod imenom ADALINE. Preobrat v razvoju sta leta 1969 v svoji knjigi povzročila Minsky in Papert [14], ki sta kritizirala dotedanje delo in pokazala pomanjkljivosti perceptrona. Z njim je namreč možno reševati le preprostejše probleme razvrščanja vzorcev, nemogoče pa je npr. realizirati preprosto binarno funkcijo (XOR). Nato je nadaljnje delo nekoliko zastalo. Ponovni razmah se je pokazal v letu 1982, ko je Hopefield [9] vpeljal nelinearno prenosno funkcijo, ter leta 1986, ko so Rumelhart, Hinton in Williams [15] predstavili algoritem za adaptacijo uteži v večnivojskih nevronskih mrežah. Po predstavitvi tega algoritma je razvoj začel napredovati z nezadržno hitrostjo, pojavljali so se različni algoritmi za učenje UNO, različne topologije, v začetku devetdesetih let pa tudi prve komercialne in industrijske aplikacije. S tem so postavili temelje učnemu pravilu vzvratnega širjenja pogreška (back- propagation), ki je še danes eno izmed najpogosteje uporabljenih. Že v naslednjem letu je avtor [8] uvedel Kolmogorov teorem o preslikavah z nevronskimi omrežji, ki govori, da je nevronsko omrežje zmožno opravljati vsako zvezno nelinearno večparametrsko preslikavo, za katero pa je potrebno najti prave uteži. Še isto leto je izšel članek, v katerem avtor [12] navaja še šest pomembnejših modelov nevronskega omrežja za razvrščanje vzorcev.\n"], "4": ["2.2 Biološki nevron", "Možganski živčni sistem je zelo zapletena tvorba. Osnovna celica živčnega sistema živih organizmov je nevron. Bistvo velike zmog1jivosti človeških možganov ni v zapletenosti delovanja posameznega nevrona, ampak v njihovih številnih medsebojnih povezavah. Kot je razvidno s slike 2.1, sestavljajo biološki nevron oziroma živčno celico: telo celice, dendriti in akson [3]. V telesu celice se odvija večina vzdrževalnih aktivnosti za obnavljanje celice. Zunanja membrana telesa ima sposobnost generiranja živčnih impulzov. Dendriti sprejemajo signale od drugih nevronov na stičnih mestih, ki se imenujejo sinapse. Ti signali se v celičnem telesu povprečijo z ostalimi podobnimi signali. Če je povprečje preko nekega kratkega časovnega intervala dovolj veliko, celica generira impulz, ki se vzdolž aksona in dalje preko sinaptičnih povezav prenese na dendrite sosednjih nevronov. Čeprav takšna preprosta funkcija v grobem opisuje delovanje nevrona, pa se v njeni dejanski implementaciji skriva zelo kompleksen elektro-kemični proces.\n"], "5": ["2.3 Umetni nevron ( matematični model )", "Zamisel delovanja umetnih nevronskih omrežij izvira iz težnje, da bi dobili model poenostavljenega delovanja pravih (bioloških) možganov. Človek si že dalj časa prizadeva razumeti, kako možgani delujejo. Na osnovi gornje razlage funkcije biološkega nevrona sta avtorja [13] sestavila matematični model nevrona ali umetni nevron. Poskušala sta čim natančneje nadomestiti strukturo in funkcijo biološkega nevrona. Nevronski sistem je zelo kompleksen, zato ni možno v popolnosti nadomestiti biološkega nevrona. Dva načina predstavitve matematičnega modela nevrona prikazuje slika 2.2.\nOsnovni matematični model je zasnovan kot nelinearni model z vhodi xj, ki so uteženi z utežmi (sinapse) wj. Telo celice je predstavljeno z nelinearno omejeno pragovno ali prenosno funkcijo F. Izhod iz nevrona je določen z enačbo:\nVsota produktov uteži in vhodov se primerja s pragom ali biasom b. Prenosna funkcija F preslika spremenljivo funkcijo v na izhod nevrona y. Najpogosteje uporabljeni prenosni funkciji sta prikazani na sliki 2.3. \nLinearna\na)\nTan.-sigmoidna\nb)\nGrafom pripadajoči funkcijski enačbi:\nF (v ) = v (2.2)\nF (v ) = tanh (v ) (2.3)\n"], "6": ["2.4 Umetno nevronsko omrežje", "Umetno nevronsko omrežje (UNO) je paralelna, distribucijska informacijska struktura, sestavljena iz elementov procesiranja - nevronov, ki so medsebojno povezani s signalnimi povezavami. Meje zmogljivosti umetnih nevronskih omrežij so postavljene precej višje kot zmogljivosti samega nevrona. Umetna nevronska omrežja lahko delujejo kot deterministični ali kot stohastični sistem. Pri determinističnih so vsi parametri in signali deterministične narave. Pri stohastičnih UNO se parametri in signali od časa do časa spreminjajo naključno. Večinoma se uporab1jajo deterministična UNO.  Model umetnega nevronskega omrežja je določen s številom in načinom povezave (topologijo) nevronov v omrežju, s prenosno funkcijo in pravilom učenja. Najpogosteje uporabljene prenosne funkcije UNO so bile predhodno že predstavljene, oglejmo si še drugi dve značilnosti.\n"], "7": ["2.4.1 Zgradba umetnega nevronskega omrežja", "Obstaja mnogo različnih načinov medsebojne povezave umetnih nevronov, ki jim pravimo tudi topologija oziroma zgradba omrežja. Glede na smer pretoka informacij delimo UNO v tri področja:\n- odprtozančna,\n- povratnozančna in\n- samoorganizirajoča omrežja. Pri odprtozančnem omrežju so nevroni razporejeni v več plasti in medsebojno povezani z enosmernimi signalnimi povezavami. V teh omrežjih je dinamika sistema preprostejša, zato ni težav s stabilnostjo. Povratnozančna omrežja imajo tudi povratne povezave nevronov. Dinamika takšnih sistemov je zahtevnejša, posebno pozornost pa je treba posvetiti stabilnosti omrežja. Samo-organizirajoča nevronska omrežja se sestojijo iz posebnih nevronskih struktur, imenovanih celice. Le-te so navadno organizirane dvodimenzionalno kot pravokotnik, trikotnik ali šestkotnik in komunicirajo direktno z drugimi samo v njihovi neposredni bližini. Vsaka celica je vzbujana s svojimi signali in signali sosednjih celic. Posamezni tipi omrežij so natančneje obravnavani kasneje v razlagi.\n"], "8": ["2.4.2 Pravila učenja umetnega nevronskega omrežja", "Učenje UNO je optimizacija uteži glede na neko kriterijsko funkcijo. Pravila učenja lahko v grobem razdelimo v dve večji skupini; v nadzorovano ter nenadzorovano učenje.\nNadzorovano učenje UNO\nPri tem načinu učenja podamo pri vsakem vhodu še ciljni oziroma želeni izhod omrežja. S tem natančno določimo preslikavo vhodov na izhode, ki jo mora opravljati UNO. Nadzorovano učenje je v bistvu implementacija optimizacijskih metod na UNO. Pri UNO imamo poleg parametrov (uteži) še vhode v mrežo, ki se tudi spreminjajo. Ti vhodi so učna množica, ki mora imeti čim več lastnosti celotne množice (vseh možnih vhodov). Funkcija, ki jo minimiziramo, je večparametrska funkcija pogreška ene ponovitve učenja z vhodi iz učne množice. Najbolj znano pravilo učenja te vrste je pravilo delta, pri katerem določimo spremembo uteži pri učenju po enačbi  ∆wij = η (d j − y j ) xi , (2.4)\nkjer pomeni η konstanto sorazmernosti učenja, yj izračunani (dejanski) izhod nevrona j, dj\npa ciljni izhod nevrona j. ∆wij predstavlja spremembo uteži med vhodom j-tega in\nizhodom i-tega nevrona. To pravilo učenja se uporablja pri enoplastnem perceptronu, za večplastni perceptron pa se uporablja pravilo z vzvratnim širjenjem pogreška, ki ga bomo predstavili v naslednjem poglavju. Varianta nadzorovanega učenja se imenuje ocenjevalno učenje. Tu UNO ne dobi pravilnega izhoda za vsak učni objekt, ampak dobi tu in tam oceno splošnega uspeha oz. napredka od zadnjega ocenjevanja. Med prvimi je ocenjevalno učenje uporabil avtor [1] za upravljanje obrnjenega nihala.\nNenadzorovano ali samo-organizirajoče učenje UNO\nPri nenadzorovanem učenju ni potrebno podati izhodov UNO niti ocen, ampak samo vhode v nevronsko omrežje. Samoorganizacija je ena od metod nenadzorovanega učenja, kjer se UNO sam organizira tako, da je sposoben producirati koristne informacije. Preslikavo določi omrežje samo. Primerno je predvsem za naloge na področju razpoznavanja vzorcev in asociativnega pomnilnika. Pri kompetitivnem ali tekmovalnem učenju nevron oz. \nnjihove povezave tekmujejo med seboj. “Zmagovalci“ si v tem procesu ojačajo uteži, medtem ko “poraženci“ svoje uteži pustijo nespremenjene ali jih celo oslabijo. Primer tekmovalnega pravila je Kohonenovo učenje. Kot smo že omenili, so UNO intenzivno raziskovalno področje, zato se obnavljajo prej našteta osnovna pravila učenja, pojavljajo pa se tudi popolnoma nova. Podobno velja tudi za topologije umetnih nevronskih omrežij.\n"], "9": ["2.5 Odprtozančno nevronsko omrežje", "V veliki večni aplikacij se, kot smo že omenili, uporablja tako imenovano nerekurentno ali »feed-forward« nevronsko omrežje. To nevronsko omrežje ima naslednja pravila oz. omejitve:\n- povezave med nevroni na istem nivoju niso dovoljene;\n- niso dovoljene povezave nazaj na prejšnje nivoje;\n- prav tako ni dovoljeno preskakovanje enega ali več nivojev pri povezavah naprej.\nKo govorimo o topologiji te vrste nevronskih omrežij, imamo v mislih število nivojev omrežja in število posameznih nevronov na teh nivojih. Ločujemo med omrežji, ki imajo enega, dva ali več nivojev nevronov. Tipični predstavniki odprtozančnega UNO so enoplastni (enonivojski) in večplastni perceptroni. Enoplastni perceptron je uporaben le za preproste primere klasifikacije vzorcev v ravnini. Mnogo uporabnejši so večplastni perceptroni. Zgradbo triplastnega perceptrona podajamo v blokovni shemi na sliki 2.4.\nKot je razvidno s slike 2.4, je večplastni perceptron razširitev enoplastnega perceptrona, ki ima izhod in skrite plasti nevronov (v našem primeru dve). Prva plast ima R vhodov. \nVsota produktov uteženih vhodov xi z utežmi wij, j-tega nevrona n-te plasti je podana z enačbo [3]  S n −1  [n ]  vj = ∑ w [ ]o [  i =1  n  ij i  n − 1]  (n = 1,2,3; j = 1,2, K , S n ) , (2.5)\nkjer pomenijo oi izhodi nevronov posamezne plasti, zato velja, da je xi[n ] = oi[n−1] , oi[0 ] := xi ,\noi[3] := y i , Sn pa je število nevronov v n-ti plasti. Za plast S0 velja, da je S0 := R (slika 2.4).\nIzhod nevrona (2.6) je definiran z vrednostjo v [jn ] (2.5) in prenosno funkcijo F j[n ] z enačbo\nIzhodi prve plasti nevronov so sočasno vhodi druge plasti, izhodi druge plasti pa so vhodi tretje plasti nevronov. Izhode tretje plasti tako zapišemo v matrični obliki  [ [ [  y = F [3] W [3]F [2 ] W [2 ]F [1] W [1]x ]]] , (2.7) kjer je x vektor vhodov UNO, y je vektor izhodov UNO, W[n] so matrike uteži posameznih plasti, F[n] pa je matrika prenosnih funkcij posameznih plasti nevronov.\n"], "10": ["2.5.1 Učenje odprtozančnih nevronskih omrežij", "Ustrezno pravilo učenja odprtozančnih omrežij je pravilo z vzvratnim širjenjem pogreška. Ta nam omogoča računanje pogreškov v posameznih skritih plasteh na podlagi pogreška izhodne plasti UNO. Glede na to vzvratno širjenje pogreška preko nevronskega omrežja je pravilo dobilo tudi to ustrezno ime. Prikazali smo ga na primeru triplastnega UNO (slika 2.5).\nPrva plast ima R vhodov, uteženih z utežmi W[1] in povezanih z S1 nevronov, druga in tretja plast vsebujeta S2 in S3 nevronov z ustrezno uteženimi povezavami. Pri učenju sodelujejo vhodni signali xi, (i = 1,2...R) in ciljni izhodni signali dj, (j = 1,2…S3). Naloga postopka učenja je prilagoditev vseh uteži UNO W[n] (n=1,2,3) tako, da bo odstopanje med ciljnimi izhodi djp in dejanskimi izhodi yjp v povprečju vseh p učnih vzorcev minimalno. Za minimiziranje funkcije vsote kvadratov pogreškov je uporabljen standardni gradientni postopek, t.j. metoda padajočih gradientov [18]. Funkcija lokalnega pogreška, ki je enaka vsoti kvadratov razlike j-tega ciljnega izhoda in j-tega dejanskega izhoda za p-ti vzorec učenja, je definirana kot\nSkupni kvadratni pogrešek je definiran z enačbo\nZa izhodno plast nevronov zapišemo, na osnovi gradientnega postopka in že omenjenega delta pravila za učenje, enačbo za spremembo uteži\nEnačbo smo sočasno v števcu in imenovalcu pomnožili z ∂v [j3] , pri čemer je na osnovi\nenačbe (2.11) izražen  S2 S2  v j = ∑ wij xi  [3]  ( [3] [3]  ) = ∑ (w[ ]o [ ] )  ij i  . (2.11)  i =1 i =1\nVhodi v izhodno (tretjo) plast nevronov xi[3] so enaki izhodom druge skrite plasti oi[2 ] . Drugi člen desne strani enačbe (2.12) definiramo z\nEnačbo (2.12) smo ponovno razširili z ∂e [jp3] . e [jp3] je razlika med ciljnim izhodom in\ndejanskim izhodom za p-ti vzorec učenja. Tretji člen desne strani izraza (2.10) je i-ti vhod tretje plasti xi[3] oziroma i-ti izhod druge plasti nevronov oi[2 ] (enačba (2.11)). Sp1ošni izraz za spremembo uteži v izhodni plasti UNO je tako podan z enačbo \n ∆wij[3] = ηδ [j3] xi[3] = ηδ [j3]oi[2 ] , (2.13)\npri čemer je po (2.14)\ndelni pogrešek za izhodno plast . Za ostali skriti plasti nevronov (prvo in drugo) je izračun δ nekoliko zahtevnejši.\nOsnovni postopek učenja s pravilom vzvratnega širjenja pogreška sestavlja naslednje zaporedje: Stopnja 1: Inicializacija vseh uteži wij[n ] in pragov bij z majhnimi začetnimi vrednostmi, ki\njih običajno izberemo naključno. (V gornjih izračunih smo prag posameznega  nevrona upoštevali kar v utežeh.) Stopnja 2: Izračun izhodov vseh nevronov za vse vhodne vzorce po enačbah (2.5 ) - (2.7)  ob upoštevanju uteži wij[n ] . To je tako imenovano računanje naprej ali »feed-\nforward« od prve plasti proti tretji plasti nevronov. Stopnja 3: Določitev ciljnih izhodov in izračun lokalnega pogreška δ [jn ] za vse plasti. Ta\ndel postopka učenja je računanje nazaj ali »back-propagation« od tretje plasti  proti prvi, kjer se vzvratno širi pogrešek. Stopnja 4: Prilagajanje uteži po iterativni enačbi  ∆wij[n ] = ηδ [jn ] x i[n ] ; (n = 3,2,1) (2.15)\nStopnja 5: Izračun novih uteži in vrnitev na stopnjo 2.  Navedene stopnje ponavljamo tako dolgo, da vsota kvadratov pogreškov  E doseže vnaprej predpisano vrednost in UNO konvergira oziroma dosežemo  predpisano število epoh učenja. Ena epoha učenja obsega enkratni izračun  izhodov iz vseh plasti nevronov, izračun vsote kvadratov pogreškov E ,  vzvratni izračun delnih pogreškov δ , izračun spremembe uteži ter novih uteži  in pragov za naslednjo epoho.\n"], "11": ["2.5.2 Izboljšana metoda vzvratnega širjenja pogreška", "V večdimenzionalnem prikazu odvisnosti vsote kvadratov pogreškov od vrednosti posameznih uteži in pragov lahko ugotavljamo lokalne in globalne minimume. Cilj postopka učenja je najti množico uteži, ki pripadajo globalnemu minimumu. Slabost klasičnega algoritma vzvratnega širjenja pogreška z gradientno metodo je v tem, da se lahko v nekaterih kombinacijah začetnih vrednosti uteži učenje nevronskega omrežja konča v lokalnem namesto v globalnem minimumu kot prikazuje slika 2.6. Na sliki je prikazana kriterijska funkcija v odvisnosti od uteži w za dvodimenzionalni primer.\nDa se izognemo lokalnemu minimumu, dopolnimo enačbo (2.15) in jo napišemo v diskretni obliki  (k +1) (k ) (k ) (k )  ∆wij[n ] = mc ∆wij[n ] + (1 − mc ) η δ [jn ] xi[n ] , (2.16)\nkjer pomeni ∆wij spremembo uteži, mc momentno konstanto, η učno konstanto, k korak\nizračuna, δ j delni pogrešek pri vzvratnem izračunu in xi vhode v nevronsko omrežje.\nEnačba (2.16) predstavlja učenje UNO z metodo padajočih gradientov z momentno konstanto. V primeru, da je momentna konstanta mc = 0, preide (2.16) v običajno metodo \npadajočih gradientov. Če je mc = 1, je nova sprememba uteži enaka stari. Dejanska vrednost mc je med nič in ena.\n"], "12": ["2.5.3 Hitrejše učenje", "V prejšnjem poglavju sta bili opisani dve učni metodi algoritma vzratnega širjenja pogreška. Metoda padajočih gradientov in metoda padajočih gradientov z momentno konstanto. Ti dve metodi sta velikokrat prepočasni za reševanje praktičnih problemov. Zato so v tem poglavju predstavljene nekatere metode učenja, ki lahko konvergirajo od deset do stokrat hitreje od predhodno predstavljenih metod.  Te hitre metode razvrščamo v dve glavni skupini. Prva uporablja hevristične tehnike, ki so razvite na osnovi analize obnašanja metode padajočih gradientov. Ena od hevrističnih modifikacij je momentna tehnika, razložena v prejšnjem poglavju, drugi dve izboljšavi sta učenje z adaptivno učno konstanto η ter odskočno ali »resilient« učenje. V drugi skupini hitrih metod uporabljamo standardne numerične optimizacijske tehnike. Pozneje sta v poglavju predstavljeni dve numerični optimizacijski metodi za učenje umetnega nevronskega omrežja: izpeljana gradientna (conjugate gradient) in kvazi-Newtonova.\n"], "13": ["2.5.4 Učenje z adaptivno učno konstanto", "Pri standardnem učenju s padajočim gradientom je učna konstanta med učenjem konstantna. Kadar izberemo preveliko učno konstanto, lahko pride do osciliranja vsote kvadratov pogreškov v odvisnosti od števila epoh učenja. Če izberemo premalo učno konstanto, pa konvergenca traja predolgo. Zato je najbolj optimalno, da se učna konstanta med učnim postopkom spreminja. Osnovna ideja metode je v uporabi gradientnega postopka za izračun dveh novih točk namesto ene [18]. Točko z manjšim pogreškom nato uporabimo v naslednji iteraciji. V k-ti iteraciji vsebuje metoda naslednje korake:\n- izračun : \npri čemer je γ majhna konstanta (na primer γ =1,7), ki jo določimo  eksperimentalno.\n- spremembo učne konstante :\n- izračun novih vrednosti uteži :\n"], "14": ["2.5.5 Odskočna metoda z vzvratnim širjenjem pogreška", "Večslojna nevronska omrežja navadno uporabljajo sigmoidne funkcije v skritih plasteh. Te funkcije pretvorijo neskončne vrednosti na vhodu v končno območje na izhodu. Njihov gradient se mora z večanjem vhodnih vrednosti bližati vrednosti nič. To povzroči problem, kadar uporabimo sigmoidne funkcije pri učenju nevronskih omrežij z metodo padajočih gradientov, ker ima lahko gradient zelo malo vrednost. Povzročena sprememba uteži je tako premajhna, čeprav so uteži daleč od optimalnih vrednosti. Princip odskočne metode z vzvratnim širjenjem pogreška je v tem, da odstrani te škodljive efekte vrednosti parcialnih odvodov. Za določitev smeri, v katero se bodo uteži spremenile, je uporabljen samo predznak odvoda [18]. Velikost odvoda ne vpliva na spremembo uteži. Kadar ima predznak v dveh iteracijah isto vrednost, pospešimo učenje s faktorjem u, nasprotno zaviramo učenje z d, kadar je predznak različen. Če je odvod nič, ostane konstanta učenja nespremenjena.\nkjer sta vrednosti konstant u > 1 in d < 1. Ta metoda v glavnem pospešuje učenje v položnih področjih funkcije pogreška kot tudi v področjih blizu lokalnega pogreška.\n"], "15": ["2.5.6 Izpeljane gradientne metode", "Osnovna metoda padajočih gradientov spremeni uteži v smeri najstrmejšega upadanja (v negativni smeri gradienta). To je smer, v kateri funkcija najhitreje upada. Izkaže pa se, da četudi funkcija v tej smeri najhitreje upada, ni nujno, da to prispeva k najhitrejšemu konvergiranju. V metodi, izpeljani iz gradientne [16], je uporabljena iskalna funkcija vzdolž sorodnih (izpeljanih) poti, ki priskrbijo hitrejše konvergiranje kot najstrmejše upadajoče smeri. Vse izpeljane gradientne metode začenjajo v prvi iteraciji z iskanjem v smeri najstrmejšega odvoda (negativni gradient).\nIskanje linije je nato izvedeno za določitev optimalne razdalje za pomik vzdolž trenutne iskalne smeri na način:  w (k +1) = w (k ) + η p (k ) . (2.22) Nato je določena naslednja iskalna smer tako, da je izpeljanka predhodni iskalni smeri. Splošni postopek za določitev nove iskalne smeri je kombinacija nove najstrmejše upadajoče smeri s predhodno iskalno smerjo:  p (k ) = − g (k ) + β (k ) p (k −1) . (2.23)\nGlede na način, kako se izračuna konstanta β k , so bile dognane številne različice izpeljank iz gradientne metode. Za Fletcher-Reeves metodo je\nŠe ena različico izpeljane gradientne metode sta predstavila Polak in Ribiere. Tako kot v Fletcher-Reeves metodi je v vsaki iteraciji določena iskalna smer. Konstanto β k za Polak- Ribiere metodo izračunamo na način: \nEnačba (2.25) predstavlja produkt prejšnje spremembe v gradientu s trenutnim gradientom, deljenega z normo kvadratov predhodnega gradienta.\n"], "16": ["2.5.7 Kvazi Newtonove metode", "Newtonova metoda je alternativa izpeljani gradientni metodi za hitro optimizacijo. Spada v družino algoritmov drugega reda, ki upoštevajo več podatkov o obliki funkcije pogreška kot zgolj o velikosti gradienta [18]. V metodah drugega reda je uporabljena kvadratna aproksimacija funkcije pogreška. Skrajšano Taylorjevo vrsto, ki aproksimira funkcijo pogreška E, lahko zapišemo kot\n∂ 2 E (w ) kjer je - Hessanova matrika parcialnih odvodov funkcije pogreška drugega reda.  ∂w 2\nGradient tako zapisane funkcije pogreška lahko izračunamo z odvajanjem (2.26), pri čemer dobimo:\nGlede na to, da iščemo minimum funkcije pogreška E, lahko (2.28) enačimo z nič in jo razrešimo, pri čemer dobimo enačbo\nkar pomeni, da je lahko minimizacija rešena v enem koraku, če imamo vnaprej izračunano Hessanovo matriko in gradient seveda ob predpostavki kvadratne funkcije pogreška. Newtonova metoda temelji na osnovi enačbe (2.29). Če označimo vektor uteži v k-ti iteraciji z w (k ) , potem je novi vektor uteži w (k +1)\n( )( ) ∂E∂w(w )  w (k +1) = w (k ) − H −1  k  . (2.30)\nNa žalost pa je računanje Hessanove matrike za odprtozančna nevronska omrežja zapleteno in dolgotrajno. Še več. Kar dejansko potrebujemo, je obrnjena Hessanova matrika. Metoda, ki uporablja poenostavljeno obliko Hessanove matrike, se imenuje kvazi- Newtonova metoda [18]. Nediagonalni elementi so postavljeni na vrednost nič in računamo samo diagonalne člene, ki so drugi odvodi oblike\nV tem primeru se (2.36) posploši za vsako komponento vektorja uteži v  ∂E (w )\n∂wi2 Čeprav kvazi-Newtonova metoda konvergira v nekaj iteracijah, pa zahteva več računanja v vsaki iteraciji kot izpeljana gradientna metoda. Poenostavljeno Hessanovo matriko dimenzije n x n, kjer je n število uteži in pragov v mreži, je potrebno v posameznih iteracijah shranjevati. Za velika omrežja je zato bolj primerno uporabiti odskočno metodo ali eno izmed izpeljanih gradientnih metod.\nEnokoračna sekantna metoda\nV tem delu obravnavamo metodo, ki upošteva informacijo drugega reda, vendar sledi raje preprostejšemu pristopu: vzeti so samo enodimenzionalni minimizacijski koraki in informacija o ukrivljenosti E v smeri spremembe, pridobljena iz trenutnega in predhodnega parcialnega odvoda E v tej smeri. Sekantna koračna metoda temelji na neodvisnih korakih optimizacije posamičnih uteži. Uporabljena je kvadratna enodimenzionalna aproksimacija funkcije pogreška. Izraz za spremembo posamezne uteži v k-tem koraku je podan z \nkjer je predpostavljeno, da je E izračunan v korakih (k-1) in k z uporabo spremembe uteži ∆wi(k −1) , pridobljene iz predhodne sekantne ali standardne gradientne metode.\n"], "abstract": ["Z uporabo umetnih nevronskih omrežij se lahko rešujejo problemi, ki se matematično težko izrazijo, hkrati pa se z analizo velike količine podatkov poiščejo relacije med vhodnimi in izhodnimi veličinami. Vendar pa je točnost rezultatov odvisna od vrste omrežja, števila skritih plasti, števila posameznih nevronov v skritih plasteh, števila učnih vzorcev, vrste prenosne funkcije in metode učenja. Karakteristična lastnost umetnih nevronskih omrežij je ta, da se lahko njihova struktura optimizira samo na podlagi izkušenj in se razlikuje od primera do primera. Namen dela je bil raziskati vpliv dejavnikov na hitrost in uspešnost učenja, povezanost nekaterih parametrov umetnih nevronskih omrežij in predstavitev ter analiza standardnih in izboljšanih metod učenja.. Uporabljeno je bilo nadzorovano učenje odprtozančnih nevronskih omrežij z učnim pravilom vzvratnega širjenja pogreška, katero se največkrat uporablja v aplikacijah na področju močnostne elektrotehnike."]}