{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcolnar/.miniconda3/envs/diploma/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "import copy\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from transformers.models.mbart.modeling_mbart import MBartLearnedPositionalEmbedding\n",
    "from transformers.models.bart.modeling_bart import BartLearnedPositionalEmbedding\n",
    "from transformers import MBartForConditionalGeneration, MBartConfig, MBart50Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "from transformers.models.longformer.modeling_longformer import LongformerSelfAttention\n",
    "from transformers.models.bart.modeling_bart import BartLearnedPositionalEmbedding\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class LongformerSelfAttentionForMBart(nn.Module):\n",
    "    def __init__(self, config, layer_id):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.d_model\n",
    "        self.longformer_self_attn = LongformerSelfAttention(\n",
    "            config, layer_id=layer_id)\n",
    "        self.output = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "\n",
    "        is_cross_attention = key_value_states is not None\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "\n",
    "        attention_mask = attention_mask.squeeze(dim=1)\n",
    "        attention_mask = attention_mask[:, 0]\n",
    "\n",
    "        is_index_masked = attention_mask < 0\n",
    "        is_index_global_attn = attention_mask > 0\n",
    "        is_global_attn = is_index_global_attn.flatten().any().item()\n",
    "\n",
    "        outputs = self.longformer_self_attn(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=None,\n",
    "            is_index_masked=is_index_masked,\n",
    "            is_index_global_attn=is_index_global_attn,\n",
    "            is_global_attn=is_global_attn,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "\n",
    "        attn_output = self.output(outputs[0])\n",
    "\n",
    "        return (attn_output,) + outputs[1:] if len(outputs) == 2 else (attn_output, None, None)\n",
    "\n",
    "\n",
    "class LongformerEncoderDecoderForConditionalGeneration(MBartForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if config.attention_mode == 'n2':\n",
    "            pass  # do nothing, use BertSelfAttention instead\n",
    "        else:\n",
    "\n",
    "            self.model.encoder.embed_positions = MBartLearnedPositionalEmbedding(\n",
    "                config.max_encoder_position_embeddings,\n",
    "                config.d_model)\n",
    "\n",
    "            self.model.decoder.embed_positions = MBartLearnedPositionalEmbedding(\n",
    "                config.max_decoder_position_embeddings,\n",
    "                config.d_model)\n",
    "\n",
    "            for i, layer in enumerate(self.model.encoder.layers):\n",
    "                layer.self_attn = LongformerSelfAttentionForMBart(\n",
    "                    config, layer_id=i)\n",
    "\n",
    "\n",
    "class LongformerEncoderDecoderConfig(MBartConfig):\n",
    "    def __init__(self, attention_window: List[int] = None, attention_dilation: List[int] = None,\n",
    "                 autoregressive: bool = False, attention_mode: str = 'sliding_chunks',\n",
    "                 gradient_checkpointing: bool = False, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            attention_window: list of attention window sizes of length = number of layers.\n",
    "                window size = number of attention locations on each side.\n",
    "                For an affective window size of 512, use `attention_window=[256]*num_layers`\n",
    "                which is 256 on each side.\n",
    "            attention_dilation: list of attention dilation of length = number of layers.\n",
    "                attention dilation of `1` means no dilation.\n",
    "            autoregressive: do autoregressive attention or have attention of both sides\n",
    "            attention_mode: 'n2' for regular n^2 self-attention, 'tvm' for TVM implemenation of Longformer\n",
    "                selfattention, 'sliding_chunks' for another implementation of Longformer selfattention\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention_window = attention_window\n",
    "        self.attention_dilation = attention_dilation\n",
    "        self.autoregressive = autoregressive\n",
    "        self.attention_mode = attention_mode\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        assert self.attention_mode in ['tvm', 'sliding_chunks', 'n2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pot do modela je morda druga\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"../results/mbart-long/checkpoint-1100\")\n",
    "\n",
    "model = LongformerEncoderDecoderForConditionalGeneration.from_pretrained(\"../results/mbart-long/checkpoint-1100/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for summarization\n",
    "max_seq_len = 10240\n",
    "\n",
    "def summarize(text, max_len):\n",
    "\n",
    "    context_tokens = ['<s>'] + tokenizer.tokenize(text) + ['</s>']\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(context_tokens) \n",
    "\n",
    "    print(f\"diploma tokenized length: {len(input_ids)}\")\n",
    "\n",
    "    if len(input_ids) < max_seq_len:   \n",
    "            while len(input_ids) < max_seq_len: \n",
    "                input_ids += [tokenizer.pad_token_id] \n",
    "    else:\n",
    "        input_ids = input_ids[:max_seq_len - 1] + [   \n",
    "            tokenizer.eos_token_id]\n",
    "\n",
    "\n",
    "    model.model.encoder.config.gradient_checkpointing = True\n",
    "    model.model.decoder.config.gradient_checkpointing = True\n",
    "\n",
    "    res_ids = model.generate(torch.tensor([input_ids]),\n",
    "                                        max_length=max_len,\n",
    "                                        num_beams=5,\n",
    "                                        no_repeat_ngram_size = 3,\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        bad_words_ids=[[tokenizer.unk_token_id]])        \n",
    "    res = tokenizer.batch_decode(res_ids.tolist(), skip_special_tokens=True)[0]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. UVOD\n",
      "\n",
      "Povzemanje daljših besedil predstavlja eno izmed težjih in manj raziskanih nalog na prodročju obdelave naravnega jezika. Delno je to zaradi tega, ker je težko definirati, kaj točno predstavlja dober pozvetek. \n",
      "Zaradi teh razlogov ne obstaja veliko tovrstnih modelov. Za razvoj modela sem se odločil, ker še ne obstaja slovenski povzemalnik dolgih besedil. \n",
      "V diplomski nalogi sem izde- lal nevronski model za povzemanje daljših besedil.  Opišem razvoj globoke nevronske mreže, ki sprejme besedilo, ga obdela in vrne povzetek.  \n",
      "Opišem kako uporabiti nevronsko mrežo arhitekture Longformer, da osnoven model prilagodimo za obdelavo daljših besedil. \n",
      "Ključna sprememba je v računanju pozornosti, kjer preidemo iz kvadratične v linearno pozornost, kar omogoča procesiranje daljših vhodov. \n",
      "Uporabimo KAS 2.0 učno množico [11] za povzemanje in Macocu [1] učno množico za vnaprejšno učenje.   \n",
      "Model evalviram kvantitativno in kvalitativno. Rezultate modela na metriki ROUGE[5] primerjam z rezultati referenčnega (angl.  baseline) modela.  \n",
      "Naključno izbrane generirane povzetke uvrstim med slabe oziroma dobre ter na podlagi ročne analize opišem modelove prednosti in slabosti.  \n",
      "V poglavju 2 najprej opišem transformer arhitekturo. Sledi opis modela BART in mBART, ki je bil na koncu tudi uporabljen za povzemanje. Nato opišem arhitekturo Longformer in kako ta omogoča linearno pozornost.\n",
      "V poglavju 3 opišem podatkove množice za učenje in vnaprejšnje učenje.  V poglavju 4 sledi opis vseh uporabljenih pristopov in orodij. \n",
      "V 5. poglavju opišem kvantitativno evalvacijo in kvalitativno evalvacijo. V 6. poglavju delo zaključim z opisom izboljšav.\n",
      " \n",
      "2. Pregled področja\n",
      "\n",
      "V poglavju opišem, kako deluje transformer arhitektura. \n",
      "Nato se osredotočim na modela BART in mBART, saj sem zadnjega uporabil tudi sam. Sledi opis Longformer arhitekture in kako ta doseže linearno pozornost.\n",
      "\n",
      "2.1\tArhitektura transformer na splošno\n",
      "\n",
      "Arhitektura globokih nevronskih mrež transformer temelji na mehanizmu pozornosti [10].  Ta omogoča, da lahko mreža pri vsaki besedi upošteva vse dele vhoda. \n",
      "Te mreže sprejemejo celoten vhod naenkrat, kar omogoča večjo stopnjo paralelnosti. Vhod je razčlenjen na žetone (angl. tokens). Te žetone predstavimo z vektorji, ki kodirajo semantiko besede. \n",
      "Doda se tudi informacija o položaju besed na vhodu. Transformer je sestavljen iz sklopa kodirnika (angl. encoder) in dekodirnika (angl. decoder). Kodirnik prvi procesira vhod. \n",
      "Naloga sklopa kodirnika je, da ugotovi, kateri deli vhoda so pomembni za druge dele vhoda. Te informacije so posredovane sklopu dekodirnika, ki generira izhod. \n",
      "Dekodirnik uporabi naučen kontekst besed in izhod kodirnika. Ta izhod je latentna predstavitev vhoda. Pri obeh delih arhitekture se uporablja mehanizem pozornosti. \n",
      "Pomemben gradnik transformerjev so glave za računanje pozornosti.  Te za vsak žeton izračunajo vložitev, ki je z po- zornostjo utežena vsota vseh ostalih žetonov, ki so relevantni za dani žeton.\n",
      "\n",
      "Vsaka glava pozornosti se nauči tri matrike:  povpraševanje Q, ključ K in vrednost V. Uteži pozornosti so produkt med matriko Q in K. Uteži normaliziramo s kvadratnim korenom dimenzije k (ključ) vektorja kot je vidno v enačbi 2.1. Shema transformer arhitekture je vidna na sliki 2.1.\n",
      "\n",
      "2.2\tModel BART\n",
      "Arhitektura modela BART [4] sledi klasičnemu transformerju.  Takim modelom pravimo tudi modeli seq2seq (zapredje v zaporedje). Seq2seq označuje modele, ki v svoji arhitekturi vsebujejo tako kodirnik kot dekodirnik. \n",
      "Ti modeli so dobri v nalogah, kjer generiramo besedilo na podlagi prejšnega, že videnega besedila. Take naloge so recimo prevajanje,  odgovarjanje na vprašanja in povzemanje.  Vhod kodirnik procesira dvosmerno (angl.  bidirectional), dekodirnik pa procesira vhod od leve proti desni. \n",
      "Ko je bil BART izdan, je bil predstavljen kot model, ki za učenje uporablja različne šumne (angl.  noising) funkcije.  Model se uči z odstranjevanjem tovrstnega šuma. Primer šumne funkcije je recimo maskiranje žetonov (angl. token masking). BART obstaja v več velikostih. Lahko ima 6 plasti ali pa 12 plasti kodirnikov in dekodirnikov.\n",
      "\n",
      "2.2.1\tmBART\n",
      "Model mBART [6] je večjezični model BART. Obstaja več različic modela. V diplomski nalogi sem uporabil mBART50, ki je bil naučen tudi na slovenskih besedilih. mBART50 je bil ustvarjen na podlagi originalnega mBART modela in bil vnaprej naučen na dodatnih 25 jezikih. \n",
      "mBART uporablja standardno seq2seq transformer arhitekturo. Sestavljen je iz 12 plasti kodirnikov in 12 plasti dekodirnikov,  kot večja različica BART modela. \n",
      "Velikost vhoda je 1024. Velikost slovarja je 250054 - za 50 jezikov. Model je primarno namenjen prevajanju besedil, vendar sem ga prilagodil za povzemanje. \n",
      "Primarni razlog za izbor tega modela je, da gre za seq2seq model, ki je bil vnaprej učen na slovenščini.\n",
      "\n",
      "2.3\tLongformer\n",
      "Modeli transformer ne morejo procesirati daljših besedil, ker je računanje pozornosti (angl.  self-attention) kvadratične časovne in prostorske zahtevnost O(n2). Longformer [2]  s prilagoditvami računanja  pozornosti doseže  linearno računsko zahtevnost O(n).  \n",
      "Slika 2.2 prikazuje primerjavo časovne in prostorske zahtevnosti polne pozornosti in Longformer prilagoditve. Prvi graf prikazuje čas obdelave skupine (angl.  batch) v odvisnosti od dolžine vhoda. \n",
      "Drugi graf pa prikazuje količino uporabljenega spomina v odvisnosti od dolžine vhoda. Težave se pojavijo predvsem pri prostorski zahtevnosti, kjer vijolična krivulja (polna pozornost) strmo narašča za razliko od rdeče krivulje (Longformer).\n",
      "\n",
      "\n",
      "2.3.1 Omejeno okno pozornosti\n",
      "Pri procesiranju daljših besedil se pojavi vprašanje ali je res potrebno, da za vsak žeton (angl.  token) izračunamo pozornost za vsako drugo besedo v besedilu. \n",
      "Ker je mehanizem pozornosti neke vrste predstavitev konteksta, lahko sklepamo, da je za večino besed bolj pomemben lokalen kontekst (oziroma pozornost) kot pa kontekst bolj oddaljenega dela besedila. \n",
      "To je pripeljalo do spoznanja, da lahko omejimo izračun pozornosti za dan žeton na lokalne žeton.  To so poimenovali premikajoče se okno pozornosti (angl. sliding window).  \n",
      "Tako smo iz n(= število vseh besed) izračunov za vsak žeton prešli na fiksnih w  izračunov pozornosti, kot je vidno na spodnji sliki 2.3b. S tem je polje pozornosti l ∗ w, kjer je l število plasti.\n",
      " \n",
      "\n",
      "2.3.2\tRazširjena pozornost\n",
      "Pojavi se vprašanje, ali je omejitev iz n  izračunov na w  izračunov morda preveč groba. Izkaže se, da lahko v modernih večplastnih arhitekturah takšna omejena pozornost skozi zaporedje plasti pokrije celotno besedilo. Vendar se da polje pozornosti še povečati, če okno pozornosti velikosti w  razredčimo (angl. dilate) z d praznimi prostori, kot je razvidno na sliki 2.3c. To pri- lagoditev so poimenovali razredčeno premikajoče se okno (angl. dilated sliding window). \n",
      "Polje pozornost se s tem poveča na l ∗ w ∗ d, kjer je d razširitev za vsako polje pozornosti.\n",
      "\n",
      "2.3.3\tDelitev na lokalno in globalno pozornost\n",
      "Zgoraj opisana pozornost vzpostavi lokalno pozornost. V nekaterih primerih mora model dostopati do celotnega konteksta v enem žetonu. \n",
      "Primer je recimo žeton CLS za model BERT, ki predstavlja celoten niz.  Zato se za nekatere žetone in pozicije doda globalna pozornost.  \n",
      "To pomeni, da taki žetoni pokrivajo pozornost do vseh drugih in vsi drugi do teh žetonov, kar pomeni da je globalna pozornost simetrična.  \n",
      "Primer globalne pozornosti v kombinaciji z omejenim premikajočim se oknom pozornosti je razviden na spodnji sliki 2.3d. Kar se tiče časovne zahtevnosti je ta še vedno  O(n), saj je tovrstna globalna pozornost mišljena le za nekaj izbranih žetonov v besedilu.\n",
      "\n",
      "3. Podatkovne množice\n",
      "V poglavju opišem uporabljene podatkovne množice za vnaprejšnje učenje in za učenje povzemanja.\n",
      "\n",
      "3.1\tVnaprejšnje učenje\n",
      "\n",
      "Čeprav na končnem modelu nisem uporabil vnaprejšnega učenja, sem ga uporabil na nekaj preizkušenih modelih, ki so imeli druge pomankljivosti, zaradi katerih jih na koncu nisem uporabil.  \n",
      "Uporaba vnaprejšnega učenja na končnem modelu je izboljšava za nadaljno delo, še posebej zaradi tega, ker je mBART [6] večjezikovni model. \n",
      "Macocu [1] je učna množica, ki je bila ustvarjena s spletnim pajkom (angl.web crawler), ki je zbral besedila domene\n",
      ".si.  Množica je bila prečiščena.  Celoten korpus vsebuje več kot 5 milijonov besedil in okoli 115 milijonov stavkov. \n",
      "TSV format, v katerem je zapisana množica, za vsak primer vsebuje URL stavka, odstavek, ID stavka znotraj dokumenta, hash kodo podobnosti in oceno kvalitete, kot je vidno na sliki 3.1.\n",
      "Množica je dostopna tudi v formatu XML.\n",
      "\n",
      "\n",
      "3.2\tKAS 2.0 - učenje\n",
      "Učna  množica  KAS  je  bila  uporabljena  za  učenje  povzemanja.\n",
      "Vsebuje slovenska akademska dela in njihove povzetke (angl. abstract). Vsebuje okoli 65 000 diplom, 16 000 magistrskih del in 1 600 doktorskih disertacij. \n",
      "Ob  času  pisanja  je  edina  učna  množica  za  povzemanje  dolgih  besedil  v slovenščini. \n",
      "Dostopna je v formatu JSON; vsako besedilo je svoja JSON datoteka. Celotno besedilo je razdeljeno na poglavja oziroma odseke, zadnji ključ JSON datoteke predstavlja povzetek, kot je razvidno na sliki 3.2.\n",
      "\n",
      "\n",
      "4. Metodologija\n",
      "V poglavju predstavim oba pristopa razvoja povzemalnika in ogrodje Hugging Face.\n",
      "\n",
      "4.1\tModel kodirnik-dekodirnik iz SloBERTe\n",
      "Glavna ideja prvega pristopa je, da izkoristim enojezikoven slovenski model RoBERTa [7] poimenovan SloBERTa [9]. \n",
      "Osnovni model SloBERTa ima vhod omejen na 512 žetonov (angl. token).  Najprej spremenimo konfiguracijo modela na željeno dolžino vhoda, v našem primeru na 20480 žetonov. \n",
      "Nastavimo, da se vhodu dodajo dopolnilni (angl.  padding) žetoni, da so vsi vhodi iste dolžine, oziroma da vhod skrajšamo na maksimalno dolžino. Nato vhod pošljemo čez vse plasti kodirnika (angl.  kodirnik) in ustvarimo spremenljivko, ki hrani instanco razreda LongformerSelfAttention. \n",
      "To spremenljivko uporabimo kot novo pozornost plasti. \n",
      "Za primer definiramo tako lokalno kot globalno pozornost. Gre za prilagojeno pozornost opisano v poglavju 2.3. S tem pridobimo daljši model, ki ga lahko učimo vnaprej (angl. pretraining), ali pa učimo neposredno na povzemanju.  \n",
      "V nalogi sem model učil vnaprej na množici Macocu [1] in evalviral.\n",
      "Koda je dostopna v obliki Google Collab Notebooka 1 in večinoma temelji na Collab Notebooku, ki prilagaja model RoBERTa. \n",
      "Prilagojen model SloBERTa, ki sprejema vhod do 20 tisoč žetonov je dostopen na Hugging Face portalu.\n",
      "Model je bil vnaprej učen na 8 tisoč primerov Macocu stavkov v 50 iteracijah.\n",
      "\n",
      "\n",
      "4.1.1\tUstvarjanje modela kodirnik-dekodirnik\n",
      "Z daljšo SloBERTo sem po zgledu članka “Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models” [8] ustvaril encoder-decoder in ga naučil na množici KAS [11]. \n",
      "Ko sem instanciral encoder-decoder model sem opazil, da so plasti pozornosti modela bile tipa RobertaAttention kot je vidno na sliki 4.1. \n",
      "Poleg tega pozornost ni bila razdeljena na globalno in lokalno. Na sliki 4.2 je za primerjavo vidna pozornost prilagojenega mBART modela. \n",
      "Težav nisem mogel odpraviti, zato sem ta pristop opustil in poskusil z modelom mBART, katerega arhitektura je že v kodirnik-dekodirnik obliki.\n",
      "\n",
      "4.2\tPovzemanje z modelom mBART\n",
      "Lewis in sod. [2], opišejo LED - Longformer Encoder Decoder.  V osnovi gre za BART model, ki BART pozornost zamenja z Longformer pozornostjo, kar omogoča razširitev vhoda.  \n",
      "Ta model sem prilagodil za učenje na učni množici KAS. To je moj končno uporabljen pristop.\n",
      "\n",
      "4.2.1\tUčenje mBART modela\n",
      "mBART sem prilagodil za daljša besedila. Proces prilagoditve je podoben prilagoditvi modela SloBERTa opisani v poglavju 4.1. Najprej sem učno množico razdelil na učno in validacijsko, kjer je 80% podatkov učnih in 20% validacijskih. \n",
      "Ustvarimo instanco razreda Dataset iz HuggingFace knjižnice, ki ponuja odprtokodne jezikovne modele in mnogo uporabnih orodij za obdelavo naravnega jezika (glej poglavje 4.3). \n",
      "Preden model naučimo je podatke potrebno iz nizov (angl. string) spremeniti v tenzorje (angl. tensor). V ta namen sem definiral funkcijo za predprocesiranje (angl. preprocessing) besedila. \n",
      "Ta podatke najprej spremenim v vektorje žetonov (angl. tokenization) in po potrebi dopolnim (angl.  padding) oziroma skrajšam (angl.  truncation), da so iste dolžine.  \n",
      "Dolžino sem omejil na približno 9 tisoč žetonov, ker je bila povprečna dolžina v učni množici okoli 7 tisoč žetonov in je bilo le nekaj procentov besedil daljših od te dolžine.  \n",
      "S Pythonovo funkcijo map() sem procesiral več skupin zapored (angl. batch). Nato lahko pripravimo parametre za učenje s pomočjo Hugging Face Seq2SeqTrainingArguments:  velikost paketa (angl. batch size) 4, stopnjo učenja 5.6e-5, faktor padca uteži (angl. weight decay) 0.5 in število epoh 1. \n",
      "Ena izmed možnih izboljšav je povečanje števila epoh učenja.  Model se je učil le na 1139 primerih, zato večja učna množica predstavlja naslednjo mogočo izboljšavo. \n",
      "Pri pisanju učne skripte sem si pomagal s kodo, ki so jo napisali avtorji članka “Summarization” [3]4 na HuggingFace portalu. Avtorji učijo za povzemanje model T5.\n",
      "\n",
      "4.3\tOrodja\n",
      "V poglavju opišem Hugging Face ogrodje, ki sem ga uporabljal tekom razvoja.\n",
      "\n",
      "4.3.1\tOgrodje Hugging Face\n",
      "Hugging Face je spletni portal, ki “ponuja orodja za grajenje, učenje in objavo modelov strojnega učenja”. \n",
      "Podpira ogrodja Tensorflow,  Pytorch in JAX. Poleg objavljenih modelov ponuja bogato knjižnico uporabnih razredov in funkcij, ki lahko pohitrijo in olajšajo razvoj jezikovnih modelov. \n",
      "Uporabil sem knjižnico za predprocesiranje besedil, ustvarjanje podatkovnega objekta (angl.  Dataset) in za učenje modela s Trainer API-jem (application programming interface).  \n",
      "Ta omogoča, da ni potrebno zapisati zanke učenja v ogrodjih, kot je Pytorch, temveč lahko v nekaj vrsticah kode poženemo učenje dane naloge.\n",
      "\n",
      "5. Evalvacija\n",
      "Pri kvantitativni evalvaciji sem uporabil metriko ROUGE [5]. Rezultate mojega modela sem primerjal z dvema osnovnima (angl. baseline) povzemalnikoma, ki za povzetek vzameta uvod oziroma zaključek besedila. \n",
      "Nato sem na vzorcu 30 generiranih povzetkov ocenil, kateri so dobri in kateri slabi. Pri kvalitativni evalvaciji sem ročno analiziral povzetke pridobljenih iz 30 naključno izbranih primerov KAS [11] ter jih primerjal z človeškimi.\n",
      "\n",
      "\n",
      "5.1\tKvantitativna evalvacija\n",
      "\n",
      "Metrika ROUGE [5] je najpogosteje uporabljena metrika za ocenjevanje mod- elov pozvemanja. Primerja prekrivanje besed med generiranim in referenčnim (ponavadi  človeškim)  povzetkom.\t\n",
      "Glavna  ideja  je,  da  priklic  in  točnost prekrivanja besed / bigramov / najdaljše sekvence harmonično utežimo v eno F1 metriko.  \n",
      "Priklic je kvocient med prekrivajočimi se besedami in dolžino referenčnega povzetka kot prikazuje enačba 5.1.  \n",
      "Točnost je kvocient med prekrivajočimi se besedami in dolžino generiranega povzetka (enačba5.2). ROUGE-1 za osnovno enoto prekrivanja vzame besede, ROUGE-2 bigrame, ROUGE-L najdaljšo prekrivajočo sekvenco po stavkih (povpreči), ROUGE-LSum pa vzame najdaljšo sekvenco preko celega povzetka.\n",
      "\n",
      "\n",
      "5.1.1\tRezultati\n",
      "\n",
      "\n",
      "V tabeli 5.1.1so prikazani rezultati prilagojenega mBART povzemalnika v primerjavi z osnovnima povzemalnikoma. Za vsako mero najprej podam rezultate mBART modela nato pa rezultat osnovnega povzemalnika, ki za povzetek vzame uvod in na koncu rezultat povzemalnika ki vzame zaključek.\n",
      "Da si lahko dobljene rezultate predstavljamo v kontekstu dolžine povzetkov so v drugi tabeli5.2prikazane povprečne dolžine povzetkov prilagojenega mBARTa, osnovnega povzemalnika z uvodom in osnovnega povzemalnika z zaključkom.\n",
      "Glede na pridobljene kvantitativne rezulate so si modeli precej blizu. Prilagojen  mBART ima  na  vseh  metrikah  nekoliko  višje vrednosti  razen na metriki ROUGE-LSum, kjer ima višjo vrednost povzemalnik z zaključkom. mBART ima preko vseh metrik v primerjavi z osnovnima modeloma višjo točnost a nižji priklic.\n",
      "To verjetno posledica tega, da so povzetki prilagojenega mBARTa precej krajši od dolžine povzetkov osnovnih povzemalnikov kot vidno v tabeli 5.2.\n",
      "Poleg metrike ROUGE sem vzel naključnih 30 primerov in ročno evalviral, kateri so dobri in kateri slabi. Rezultati so prikazani v tabeli 5.3.\n",
      "Utemeljitve za uvrstitev primerov so vidne v dodatku A.\n",
      " \n",
      "\n",
      "5.2\tKvalitativna analiza naključnih izbranih primerov\n",
      "Izbral sem 30 naključnih testnih besedil iz KAS [11] množice. Nato sem primerjal generirani povzetek s človeškim.  \n",
      "Osredotočil sem se na pravilnost jezika, pravilnost in pomembnost predstavljenih informacij ter na odvečne oziroma pomankljive informacije generiranega povzetka. \n",
      "Po analizi sem zaključil, da je jezik generiranih povzetkov bil z izjemo nekaj primerov pravilen in tekoč. Največje težave je imel model z izpuščanjem pomembnih informacij. \n",
      "Trend, ki se je tekom analize velikokrat pojavil je, da so kratki povzetki izpustili pomembne informacije, kljub temu pa so jih precej površno povzeli (v enem ali dveh stavkih). \n",
      "Zaradi tega razloga je bila večina krajših povzetkov uvrščenih kot slabih. V nasprotju pa se je večina daljših povzetkov izkazala za dobre ob občasni redundanci informacij.  \n",
      "Z izjemo nekaj primerov so bili predstavljeni podatki povzetkov pravilni. Nekaj težav ima model s številskimi podatki in statistikami, večji del besedila se ponavadi vrti okoli teoretičnega dela besedila. \n",
      "Analiza 30 povzetkov je dostopna v dodatku A.\n",
      "\n",
      "5.2.1\tPrimer dobrega povzetka\n",
      "Izbran primer predstavlja dober povzetek, kjer ima povzetek pravilen jezik, zajame pomembne informacije in ne vsebuje napačnih podatkov.\n",
      "\n",
      "•\tČloveški: Učitelj si želi poučevati na način, ki bi otroku dal kakovostno in hkrati tudi dolgotrajno znanje. \n",
      "Zato so zelo pomembna prva leta šolanja, saj v tem obdobju usvoji pojme in jih kasneje tudi nadgradi. \n",
      "Za diplomsko delo sem si izbrala Matematične zgodbe, saj me je zanimalo, kakšno matematično znanje imajo učenci drugega razreda ob\n",
      "zaključku šolskega leta. Želela sem, da otrok preko zgodbe pokaže usvojeno znanje, saj sem v nalogah zajela temeljne cilje iz matematičnih vsebin za drugi razred. \n",
      "V teoretičnem delu diplomskega dela sem predstavila, kakšna mora biti motivacija pri pouku matematike in na kakšen način mora učitelj poučevati matematiko. \n",
      "Kot učitelji se moramo zavedati, da je bistveni dejavnik poučevanja in s tem tudi učenja matematike motivacija. \n",
      "Vsak problem, ki ga predstavimo otroku, mora izhajati iz njegovih lastnih spoznavnih potreb in radovednosti. \n",
      "Pomembno je tudi, da izhajamo iz konkretnega sveta, saj bo tako otrok matem- atiko doživljal kot koristno tudi v ponudimo nekaj drugačnega, nekaj nevsakdanjega. \n",
      "Predstavljajo namreč bolj zanimiv način reševanja nalog za otroke, ki je tudi zelo poučen in zabaven. Res pa je, da zahteva od učitelja veliko časa in priprav.\n",
      "Učitelj pa mora imeti za doseganje željenih ciljev na voljo tudi veliko didaktičnih pripomočkov. želela preveriti,  ali  so  otroci  usvojili  temeljne  cilje  iz  matematičnih  vsebin  v drugem razredu. \n",
      "Ugotovila sem, da je bila predstavljena oblika preverjanja ustrezna, saj so se v tem obdobju že naučili tekočega branja in branja z razumevanjem. \n",
      "Matematične zgodbe so jih zelo dobro motivirale, saj takšen način poučevanja matematike za otroke predstavlja nekaj zanimivega in drugačnega, a hkrati tudi poučnega.\n",
      "\n",
      "•\tGenerirani: V prvem triletju osnovnega šolanja matematika razvija dejavnosti, ki so tesno povezane z življenjskim procesom otroka. Tako otrok razvija različne oblike mišljenja. \n",
      "Temelji na reševanju problemov, v katere mora biti otrok aktivno vključen, saj z njimi odkriva svoje sposobnosti in spretnosti.  \n",
      "V obdobju je otroku najpomembnejša dejavnost igra, zato je zelo pomembno, da učitelj organizira pouk preko najrazličnejših iger, opazovanja in tudi izkušenjskega učenja.  \n",
      "Na ta način bo otroku približal posamezne matematične vsebine. Pomembno je tudi, da pri pouku otroku ponudi čim več različnega didaktičnega materiala in pripomočkov. \n",
      "Tako jih lahko uporabi vedno, kadar začuti potrebo, si to želi ali pa to nujno potrebuje.\n",
      "Učitelj mora vedeti in razumeti, da učenci na začetni stopnji miselnega razvoja še potrebujejo pomoč ponazoril.\n",
      "Zato mora upoštevati individualne sposobnosti učencev in uveljaviti načelo individualizacije. Tako mora pouk matematike nuditi učencem razna matematična znanja, da lažje rešujejo različne problemske situacije.\n",
      "Bistvo je naučiti otroka misliti, ki je tudi zelo poučen in zabaven. Vendar zahteva od učitelja veliko priprav in organizacije. Preko matematičnih zgodb lahko učence uvajamo v nekodoločeno snov, jo utrjujemo in hkrati tudi preverjamo.\n",
      "Če učenci še ne znajo brati ali so šele v procesu učenja, lahko zgodbo bere učitelj. Učenci so pri načinu dela bolj motivirani za delo in imajo usmerjeno pozornost. \n",
      "Predstavljena oblika preverjanja je bila ustrezna za učence 2. razreda, saj v obdobju že dokaj dobro obvladajo branje in branje z razumevanjem.\n",
      "\n",
      "•\tInterpretacija: Jezik generiranega povzetka je pravilen z izjemo enega stavka - “Bistvo je naučiti otroka misliti,  ki je tudi zelo poučen in zabaven”. \n",
      "Generirani povzetek vsebuje vse pomembne informacije iz človeškega. Ponazarja pomembnost matematičnih zgodb kot didaktičnega pripomočka. Predstavlja zaključeno celoto in zajame dovolj informacij.\n",
      "\n",
      "5.2.2\tPrimer slabega povzetka\n",
      "Izbran primer predstavlja primer, kjer je šlo vse narobe.  Jezik vsebuje napake, stavki niso smiselni, in manjkajo pomembne informacije.\n",
      "\n",
      "•\tČloveški: Diplomsko delo obravnava uporabo terapevtske komunikacije med medicinsko sestro in stanovalcem v institucionalnem varstvu. \n",
      "Predstavljeni so elementi terapevtske komunikacije, njihove prednosti in pomanjkljivosti  ter  tehnike terapevtske komunikacije. \n",
      "Predvsem je poudarek na terapevtski komunikaciji, ki se uporablja v institucionalnem varstvu, saj zajema različno starostno strukturo stanovalcev z različnimi obolenji. \n",
      "V empiričnem delu diplomskega dela so zbrani in predstavljeni rezultati raziskave, ki je bila izvedena v enem od domov za starejše v podravski regiji. \n",
      "Z raziskavo smo želeli ugotoviti uporabo terapevtske komunikacije v institucionalnem varstvu s stanovalcem, zato smo vključili v raziskavo zaposlene izvajalce zdravstvene nege. \n",
      "Ugotovili smo, da izvajalci zdravstvene nege uporabljajo terapevtsko komunikacijo pri izvajanju aktivnosti zdravstvene nege. \n",
      "Rezultati so pokazali dejavnike, ki vplivajo na izvajanje terapevtske komunikacije v institucionalnem varstvu, ti so:  pomanjkanje časa, preveč delovnih obveznosti in zdravstvene omejitve stanovalcev.\n",
      "\n",
      "•\tGenerirani: nalogi s področja kemijskih vsebin. Problem je, da kljub vloženemu trudu in času, ki smo ga namenili za zbiranje informacij, raziskovanje in pisanje seminarskih nalog, te naloge po interni predstavitvi postanejo neuporabne in pozabljene ter velikokrat shranjene le v metodikovem arhivu. \n",
      "Da naše vloženo delo ne bi bilo pozabljeno, sva se mentorico domislili priprave internega študijskega glasila Kemko, ki bo služilo kot raziskovalni vzorec diplomskega dela za potrebe evalvacijske (samo) analize in kot prispevki internih študijskih glasil Kemko.”\n",
      "\n",
      "•\tInterpretacija: To je primer zelo slabega povzetka, saj jezik vsebuje napake in stavki niso umeščeni v kontekst. Celoten generiran povzetek je bralcu nesmiseln.\n",
      "\n",
      "6. Zaključek\n",
      "Razvil sem model za povzemanje daljših besedil. Po zgledu dela Lewis et al[2] sem spremenil plasti pozornosti, da se namesto kvadratične spremenijo v linearno časovno in prostorsko kompleksnost. \n",
      "Prilagodil sem model mBART, ki je bil med drugim naučen na slovenskem jeziku. mBART sem učil povzemati daljša besedila (do približno 9000 besed oz.  žetonov).  \n",
      "Uporabil sem učno množico KAS 2.0[11], ki vsebuje akademska dela in njihove povzetke. \n",
      "Kvantitativno sem model ovrednotil z metriko ROUGE ter primerjal rezultate modela z osnovnima (angl. baseline) modeloma, ki sta povzetek besedila vzel uvod oziroma zaključek.  \n",
      "Model je bil kvantitativno nekoliko boljši od osnovnih.  Nato sem naključno izbral 30 primerov in jih razvrstil kot dobre oziroma slabe; 11 primerov se je izkazalo kot dobrih in 19 kot slabih. \n",
      "Za kvalitativno evalvacijo sem 30 primerov ročno analiziral.  Ugotovil sem, da model nima težav z pravilnostjo jezika in pravilnostjo podatkov. \n",
      "Težave se pojavijo pri krajših povzetkih, kjer pride do izpuščanja pomembnih informacij. Glavna izboljšava modela bi bilo učenje z več primeri in dalj časa. \n",
      "Model mBART bi lahko tudi vnaprej učili na slovenski učni množici Macocu. Zanimivo bi bilo tudi uspešno podaljšati model SloBERTa in primerjati modela.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diploma = \"\"\n",
    "with open('./diploma.txt') as f:\n",
    "    diploma = f.read()\n",
    "    print(diploma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diploma tokenized length: 6259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'V diplomski nalogi opišem razvoj globoke nevronske mreže, ki sprejme besedilo, ga obdela in vrne povzetek. Opišem kako uporabiti nevronsko mrežo arhitekture Longformer, da osnoven model prilagodimo za obdelavo daljših besedil. Ključna sprememba je v računanju pozornosti, kjer preidemo iz kvadratične v linearno pozornost. Uporabimo KAS 2.0 učno množico [11] za povzetek in Macocu [1] učni množico za vnaprejšno učenje. Model evalviram kvantitativno in kvalitativno. Rezultate modela na metriki ROUGE[5] primerjam z rezultati referenčnega (angl. baseline) modela.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(diploma, max_seq_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('diploma': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7dd6fc77cfe7929ab6e14567fa546fdaf6d03841859679a01b012c77d7e8b345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
