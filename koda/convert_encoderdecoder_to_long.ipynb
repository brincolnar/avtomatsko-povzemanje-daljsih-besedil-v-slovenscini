{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcolnar/.miniconda3/envs/diploma/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "import copy\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from transformers.models.mbart.modeling_mbart import MBartLearnedPositionalEmbedding\n",
    "from transformers.models.bart.modeling_bart import BartLearnedPositionalEmbedding\n",
    "from transformers import MBartForConditionalGeneration, MBartConfig, MBart50Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "from transformers.models.longformer.modeling_longformer import LongformerSelfAttention\n",
    "from transformers.models.bart.modeling_bart import BartLearnedPositionalEmbedding\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class LongformerSelfAttentionForMBart(nn.Module):\n",
    "    def __init__(self, config, layer_id):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.d_model\n",
    "        self.longformer_self_attn = LongformerSelfAttention(config, layer_id=layer_id)\n",
    "        self.output = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "\n",
    "        is_cross_attention = key_value_states is not None\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "\n",
    "        attention_mask = attention_mask.squeeze(dim=1)\n",
    "        attention_mask = attention_mask[:,0]\n",
    "\n",
    "        is_index_masked = attention_mask < 0\n",
    "        is_index_global_attn = attention_mask > 0\n",
    "        is_global_attn = is_index_global_attn.flatten().any().item()\n",
    "\n",
    "        outputs = self.longformer_self_attn(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=None,\n",
    "            is_index_masked=is_index_masked,\n",
    "            is_index_global_attn=is_index_global_attn,\n",
    "            is_global_attn=is_global_attn,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "\n",
    "        attn_output = self.output(outputs[0])\n",
    "\n",
    "        return (attn_output,) + outputs[1:] if len(outputs) == 2 else (attn_output, None, None)\n",
    "\n",
    "class LongformerEncoderDecoderForConditionalGeneration(MBartForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        print(f'config: {config}')\n",
    "        super().__init__(config)\n",
    "        \n",
    "        print(f'before if statement')\n",
    "\n",
    "        if config.attention_mode == 'n2':\n",
    "            pass  # do nothing, use BertSelfAttention instead\n",
    "        else:\n",
    "\n",
    "            print('instantiating MBartLearnedPositionalEmbedding for encoder...')\n",
    "\n",
    "            self.model.encoder.embed_positions = MBartLearnedPositionalEmbedding(\n",
    "                config.max_encoder_position_embeddings,\n",
    "                config.d_model)\n",
    "\n",
    "            print('instantiating MBartLearnedPositionalEmbedding for decoder...')\n",
    "            self.model.decoder.embed_positions = MBartLearnedPositionalEmbedding(\n",
    "                config.max_decoder_position_embeddings, \n",
    "                config.d_model)\n",
    "\n",
    "            print('replacing attention with long attention...')\n",
    "            for i, layer in enumerate(self.model.encoder.layers):\n",
    "                layer.self_attn = LongformerSelfAttentionForMBart(config, layer_id=i)\n",
    "\n",
    "class LongformerEncoderDecoderConfig(MBartConfig):\n",
    "    def __init__(self, attention_window: List[int] = None, attention_dilation: List[int] = None,\n",
    "                 autoregressive: bool = False, attention_mode: str = 'sliding_chunks',\n",
    "                 gradient_checkpointing: bool = False, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            attention_window: list of attention window sizes of length = number of layers.\n",
    "                window size = number of attention locations on each side.\n",
    "                For an affective window size of 512, use `attention_window=[256]*num_layers`\n",
    "                which is 256 on each side.\n",
    "            attention_dilation: list of attention dilation of length = number of layers.\n",
    "                attention dilation of `1` means no dilation.\n",
    "            autoregressive: do autoregressive attention or have attention of both sides\n",
    "            attention_mode: 'n2' for regular n^2 self-attention, 'tvm' for TVM implemenation of Longformer\n",
    "                selfattention, 'sliding_chunks' for another implementation of Longformer selfattention\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention_window = attention_window\n",
    "        self.attention_dilation = attention_dilation\n",
    "        self.autoregressive = autoregressive\n",
    "        self.attention_mode = attention_mode\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        assert self.attention_mode in ['tvm', 'sliding_chunks', 'n2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_long_model(save_model_to, base_model, tokenizer_name_or_path, attention_window, max_pos):\n",
    "    tokenizer = MBart50Tokenizer.from_pretrained(tokenizer_name_or_path, model_max_length=max_pos)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(base_model)\n",
    "    config = LongformerEncoderDecoderConfig.from_pretrained(base_model)\n",
    "\n",
    "    model.config = config\n",
    "\n",
    "    # in BART attention_probs_dropout_prob is attention_dropout, but LongformerSelfAttention\n",
    "    # expects attention_probs_dropout_prob, so set it here\n",
    "\n",
    "    config.attention_probs_dropout_prob = config.attention_dropout\n",
    "    config.architectures = ['LongformerEncoderDecoderForConditionalGeneration', ]\n",
    "\n",
    "    # extend position embeddings\n",
    "    tokenizer.model_max_length = max_pos\n",
    "    tokenizer.init_kwargs['model_max_length'] = max_pos\n",
    "    current_max_pos, embed_size = model.model.encoder.embed_positions.weight.shape\n",
    "    assert current_max_pos == config.max_position_embeddings + 2\n",
    "\n",
    "    config.max_encoder_position_embeddings = max_pos\n",
    "    config.max_decoder_position_embeddings = config.max_position_embeddings\n",
    "    del config.max_position_embeddings\n",
    "    max_pos += 2  # NOTE: BART has positions 0,1 reserved, so embedding size is max position + 2\n",
    "    assert max_pos >= current_max_pos\n",
    "\n",
    "    # allocate a larger position embedding matrix for the encoder\n",
    "    new_encoder_pos_embed = model.model.encoder.embed_positions.weight.new_empty(max_pos, embed_size)\n",
    "\n",
    "\n",
    "    print(f'new_encoder_pos_embed: {new_encoder_pos_embed}')\n",
    "    print(f'new_encoder_pos_embed.shape: {new_encoder_pos_embed.shape}')\n",
    "\n",
    "    # copy position embeddings over and over to initialize the new position embeddings\n",
    "    k = 2\n",
    "    step = current_max_pos - 2\n",
    "    while k < max_pos - 1:\n",
    "        new_encoder_pos_embed[k:(\n",
    "            k + step)] = model.model.encoder.embed_positions.weight[2:]\n",
    "        k += step\n",
    "        \n",
    "    model.model.encoder.embed_positions.weight.data = new_encoder_pos_embed\n",
    "\n",
    "    # allocate a larger position embedding matrix for the decoder\n",
    "    # new_decoder_pos_embed = model.model.decoder.embed_positions.weight.new_empty(max_pos, embed_size)\n",
    "    # # copy position embeddings over and over to initialize the new position embeddings\n",
    "    # k = 2\n",
    "    # step = current_max_pos - 2\n",
    "    # while k < max_pos - 1:\n",
    "    #     new_decoder_pos_embed[k:(k + step)] = model.model.decoder.embed_positions.weight[2:]\n",
    "    #     k += step\n",
    "    # model.model.decoder.embed_positions.weight.data = new_decoder_pos_embed\n",
    "\n",
    "    # replace the `modeling_bart.SelfAttention` object with `LongformerSelfAttention`\n",
    "\n",
    "    config.attention_window = [attention_window] * config.num_hidden_layers\n",
    "    config.attention_dilation = [1] * config.num_hidden_layers\n",
    "\n",
    "    for i, layer in enumerate(model.model.encoder.layers):\n",
    "        longformer_self_attn_for_mbart = LongformerSelfAttentionForMBart(config, layer_id=i)\n",
    "\n",
    "        longformer_self_attn_for_mbart.longformer_self_attn.query = layer.self_attn.q_proj\n",
    "        longformer_self_attn_for_mbart.longformer_self_attn.key = layer.self_attn.k_proj\n",
    "        longformer_self_attn_for_mbart.longformer_self_attn.value = layer.self_attn.v_proj\n",
    "\n",
    "        longformer_self_attn_for_mbart.longformer_self_attn.query_global = copy.deepcopy(\n",
    "            layer.self_attn.q_proj)\n",
    "        longformer_self_attn_for_mbart.longformer_self_attn.key_global = copy.deepcopy(\n",
    "            layer.self_attn.k_proj)\n",
    "        longformer_self_attn_for_mbart.longformer_self_attn.value_global = copy.deepcopy(\n",
    "            layer.self_attn.v_proj)\n",
    "\n",
    "        longformer_self_attn_for_mbart.output = layer.self_attn.out_proj\n",
    "\n",
    "        layer.self_attn = longformer_self_attn_for_mbart\n",
    "\n",
    "    logger.info(f'saving model to {save_model_to}')\n",
    "    model.save_pretrained(save_model_to)\n",
    "    tokenizer.save_pretrained(save_model_to, None)\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model arguments: \n",
      "save_model_to=./tmp/20k/mbart-long, base_model=facebook/mbart-large-50, tokenizer_name_or_path=facebook/mbart-large-50, attention_window=512, max_pos=20480\n",
      "creating new model...\n",
      "new_encoder_pos_embed: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "new_encoder_pos_embed.shape: torch.Size([20482, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:saving model to ./tmp/20k/mbart-long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: MBartConfig {\n",
      "  \"_name_or_path\": \"./tmp/20k/mbart-long\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dilation\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_mode\": \"sliding_chunks\",\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"autoregressive\": false,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 20480,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "before if statement\n",
      "instantiating MBartLearnedPositionalEmbedding for encoder...\n",
      "instantiating MBartLearnedPositionalEmbedding for decoder...\n",
      "replacing attention with long attention...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "save_model_to = './tmp/20k/mbart-long'# './tmp/mbart-long'\n",
    "base_model = 'facebook/mbart-large-50'\n",
    "tokenizer_name_or_path = 'facebook/mbart-large-50'\n",
    "attention_window = 512\n",
    "max_pos =  20480 # 16384 # mutiple of origin maximum encoder positions ?\n",
    "\n",
    "print('new model arguments: ')\n",
    "print(f'save_model_to={save_model_to}, base_model={base_model}, tokenizer_name_or_path={tokenizer_name_or_path}, attention_window={attention_window}, max_pos={max_pos}')\n",
    "print('creating new model...')\n",
    "\n",
    "create_long_model(\n",
    "    save_model_to=save_model_to,\n",
    "    base_model=base_model,\n",
    "    tokenizer_name_or_path=tokenizer_name_or_path,\n",
    "    attention_window=attention_window,\n",
    "    max_pos=max_pos\n",
    ")\n",
    "\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(save_model_to)\n",
    "model = LongformerEncoderDecoderForConditionalGeneration.from_pretrained(save_model_to)\n",
    "\n",
    "model.model.encoder.config.gradient_checkpointing = True\n",
    "model.model.decoder.config.gradient_checkpointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongformerEncoderDecoderForConditionalGeneration(\n",
      "  (model): MBartModel(\n",
      "    (shared): Embedding(250054, 1024, padding_idx=1)\n",
      "    (encoder): MBartEncoder(\n",
      "      (embed_tokens): Embedding(250054, 1024, padding_idx=1)\n",
      "      (embed_positions): MBartLearnedPositionalEmbedding(20482, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): MBartDecoder(\n",
      "      (embed_tokens): Embedding(250054, 1024, padding_idx=1)\n",
      "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  16384\n",
      "Rimsko cesarstvo (latinsko Imperivm Romanvm, grško Βασιλεία τῶν ὄωμαίων, Basileía tōn Rhōmaíōn) je bilo obdobje starega Rima, ki je sledilo Rimski republiki. Kot država je obsegalo veliko ozemlje okoli Sredozemskega morja v Evropi, severni Afriki in zahodni Aziji. V cesarstvu so vladali cesarji. Od začetka vladavine cesarja Avgusta do vojaške anarhije v 3. stoletju je bila država principat z Italijo kot metropolo provinc in Rimom kot edino prestolnico (27 pr. n. št. - 286 n. m.). Po krizi 3. stoletja je bilo cesarstvo razdeljeno v Zahodno rimske cesarstvo in Vzhodno rimsko cearstvo. Slednje je znano tudi kot Bizantinsko cesarstvo. Cesarstvi sta imeli vsako svojega cesarja. Uradna prestolnica obeh cesarstev je do leta 476 ostal Rim. Tisto leto so Raveno zasedli Odoakerjevi Ostrogoti in odstavili zadnjega zahodnorimskega cesarja Romula Avgust.\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 512 * 32\n",
    "\n",
    "def summarize(text, max_len):\n",
    "\n",
    "    context_tokens = ['<s>'] + tokenizer.tokenize(text) + ['</s>']\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(context_tokens) \n",
    "\n",
    "    if len(input_ids) < max_seq_len:   \n",
    "            while len(input_ids) < max_seq_len: \n",
    "                input_ids += [tokenizer.pad_token_id] \n",
    "    else:\n",
    "        input_ids = input_ids[:max_seq_len - 1] + [   \n",
    "            tokenizer.eos_token_id]\n",
    "\n",
    "    print('input_ids: ',len(input_ids))\n",
    "\n",
    "\n",
    "    model.model.encoder.config.gradient_checkpointing = True\n",
    "    model.model.decoder.config.gradient_checkpointing = True\n",
    "\n",
    "    res_ids = model.generate(torch.tensor([input_ids]),\n",
    "                                        max_length=max_len,\n",
    "                                        num_beams=5,\n",
    "                                        no_repeat_ngram_size = 3,\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        bad_words_ids=[[tokenizer.unk_token_id]])        \n",
    "    res = tokenizer.batch_decode(res_ids.tolist(), skip_special_tokens=True)[0]\n",
    "    \n",
    "    return print(res)\n",
    "\n",
    "summarize(\"Rimsko cesarstvo (latinsko Imperivm Romanvm, grško Βασιλεία τῶν Ῥωμαίων, Basileía tōn Rhōmaíōn) je bilo obdobje starega Rima, ki je sledilo Rimski republiki. Kot država je obsegalo veliko ozemlje okoli Sredozemskega morja v Evropi, severni Afriki in zahodni Aziji. V cesarstvu so vladali cesarji. Od začetka vladavine cesarja Avgusta do vojaške anarhije v 3. stoletju je bila država principat z Italijo kot metropolo provinc in Rimom kot edino prestolnico (27 pr. n. št. - 286 n. št.). Po krizi 3. stoletja je bilo cesarstvo razdeljeno v Zahodno rimsko cesarstvo in Vzhodno rimsko cesarstvo. Slednje je znano tudi kot Bizantinsko cesarstvo. Cesarstvi sta imeli vsako svojega cesarja. Uradna prestolnica obeh cesarstev je do leta 476 ostal Rim. Tisto leto so Raveno zasedli Odoakerjevi Ostrogoti in odstavili zadnjega zahodnorimskega cesarja Romula Avgusta, zato so cesarske insignije prenesli v Konstantinopel. S sprejetjem krščanstva kot državne vere Rimskega cesarstva leta 380 in padcem Zahodnega rimskega cesarstva se je končalo obdobje klasične antike in začel srednji vek. Ti dogodki in postopna helenizacija Vzhodnega rimskega cesarstva so razlog, da zgodovinarji srednjeveško Rimsko cesarstvo, ki je ostalo v vzhodnih rimskih provincah, imenujejo Bizantinsko cesarstvo. Prvi dve stoletji cesarstva sta bili obdobje stabilnosti in razcveta brez primere, znano kot Pax Romana (rimski mir). V 3. stoletju je cesarstvo doživelo krizo, ki je ogrozila njegov obstoj, saj sta se Galsko in Palmirsko cesarstvo odcepila. Na prestolu se je zvrstila vrsta kratkoživih cesarjev, pogosto legionarjev in pogosto več hkrati. Cesarstvo je ponovno združil Avrelijan (270–275). Dioklecijan je poskusil cesarstvo stabilizirati in ga je leta 286 razdelil na latinski zahod in grški vzhod. V 4. stoletju so se po Milanskem ediktu iz leta 313 začeli na vplivne državne položaje vzpenjati kristjani. Kmalu zatem se je začelo obdobje velikih selitev, obsežnih vpadov germanskih ljudstev in Hunov pod vodstvom Atile, kar je pripeljalo do propada Zahodnega rimskega cesarstva. S padcem Ravene pod germanske Herule in z Odoakerjevo odstavitvijo cesarja Romula Avgusta leta 476 je Zahodno rimsko cesarstvo dokončno propadlo, kar je formalno potrdil vzhodnorimski cesar Zenon leta 480. Nekatere države na ozemljih nekdanjega Zahodnega rimskega cesarstva so se kasneje imele za njegove dediče in so se potegovale za vrhovno oblast rimskih cesarjev. Najpomembnejše med njimi je bilo Sveto rimsko cesarstvo. Vzhodno rimsko cesarstvo je preživelo še celo tisočletje, dokler niso Konstantinopla leta 1453 zavzeli osmanski Turki pod vodstvom sultana Mehmeda II.[op 4] Zaradi velikega ozemlja in dolgega obstoja Rimskega cesarstva so rimske upravne prakse in kultura močno in trajno vplivale na razvoj jezika, religije, umetnosti, arhitekture, filozofije, prava in oblik vladanja ne samo na ozemlju, ki ga je obsegalo, temveč tudi daleč preko meja. Jezik Rimljanov se je razvil v romanske jezike srednjeveškega in modernega sveta, medtem ko je klasična grščina postala jezik Vzhodnega rimskega cesarstva. Sprejetje krščanstva v cesarstvu je privedlo do oblikovanja srednjeveškega krščanstva. Grška in rimska umetnost sta močno vplivali na italijansko renesanso. Rimska arhitekturna tradicija je služila kot osnova za romansko, renesančno in neoklasično arhitekturo, močno pa je vplivala tudi na islamsko arhitekturo. Rimsko pravo ima naslednike v mnogih današnjih pravnih sistemih, na primer v Napoleonovem zakoniku, pa tudi v francoskem in italijanskem sodobnem pravu; rimske republiške institucije so pustile trajno zapuščino, ki je vplivala na srednjeveške italijanske mestne republike in države, pozneje pa tudi na politično ureditev v ZDA in drugih modernih \", 512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('diploma': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7dd6fc77cfe7929ab6e14567fa546fdaf6d03841859679a01b012c77d7e8b345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
