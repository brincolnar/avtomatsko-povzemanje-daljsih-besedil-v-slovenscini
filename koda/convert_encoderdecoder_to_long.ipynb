{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bcolnar/.miniconda3/envs/diploma/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "import copy\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from transformers.models.mbart.modeling_mbart import MBartLearnedPositionalEmbedding\n",
    "from transformers.models.bart.modeling_bart import BartLearnedPositionalEmbedding\n",
    "from transformers import MBartForConditionalGeneration, MBartConfig, MBart50Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "from transformers.models.longformer.modeling_longformer import LongformerSelfAttention\n",
    "from transformers.models.bart.modeling_bart import BartLearnedPositionalEmbedding\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class LongformerSelfAttentionForMBart(nn.Module):\n",
    "    def __init__(self, config, layer_id):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.d_model\n",
    "        self.longformer_self_attn = LongformerSelfAttention(config, layer_id=layer_id)\n",
    "        self.output = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "\n",
    "        is_cross_attention = key_value_states is not None\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "\n",
    "        attention_mask = attention_mask.squeeze(dim=1)\n",
    "        attention_mask = attention_mask[:,0]\n",
    "\n",
    "        is_index_masked = attention_mask < 0\n",
    "        is_index_global_attn = attention_mask > 0\n",
    "        is_global_attn = is_index_global_attn.flatten().any().item()\n",
    "\n",
    "        outputs = self.longformer_self_attn(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=None,\n",
    "            is_index_masked=is_index_masked,\n",
    "            is_index_global_attn=is_index_global_attn,\n",
    "            is_global_attn=is_global_attn,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "\n",
    "        attn_output = self.output(outputs[0])\n",
    "\n",
    "        return (attn_output,) + outputs[1:] if len(outputs) == 2 else (attn_output, None, None)\n",
    "\n",
    "class LongformerEncoderDecoderForConditionalGeneration(MBartForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        print(f'config: {config}')\n",
    "        super().__init__(config)\n",
    "        \n",
    "        print(f'before if statement')\n",
    "\n",
    "        if config.attention_mode == 'n2':\n",
    "            pass  # do nothing, use BertSelfAttention instead\n",
    "        else:\n",
    "\n",
    "            print('instantiating MBartLearnedPositionalEmbedding for encoder...')\n",
    "\n",
    "            self.model.encoder.embed_positions = MBartLearnedPositionalEmbedding(\n",
    "                config.max_encoder_position_embeddings,\n",
    "                config.d_model)\n",
    "\n",
    "            print('instantiating MBartLearnedPositionalEmbedding for decoder...')\n",
    "            self.model.decoder.embed_positions = MBartLearnedPositionalEmbedding(\n",
    "                config.max_decoder_position_embeddings, \n",
    "                config.d_model)\n",
    "\n",
    "            print('replacing attention with long attention...')\n",
    "            for i, layer in enumerate(self.model.encoder.layers):\n",
    "                layer.self_attn = LongformerSelfAttentionForMBart(config, layer_id=i)\n",
    "\n",
    "class LongformerEncoderDecoderConfig(MBartConfig):\n",
    "    def __init__(self, attention_window: List[int] = None, attention_dilation: List[int] = None,\n",
    "                 autoregressive: bool = False, attention_mode: str = 'sliding_chunks',\n",
    "                 gradient_checkpointing: bool = False, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            attention_window: list of attention window sizes of length = number of layers.\n",
    "                window size = number of attention locations on each side.\n",
    "                For an affective window size of 512, use `attention_window=[256]*num_layers`\n",
    "                which is 256 on each side.\n",
    "            attention_dilation: list of attention dilation of length = number of layers.\n",
    "                attention dilation of `1` means no dilation.\n",
    "            autoregressive: do autoregressive attention or have attention of both sides\n",
    "            attention_mode: 'n2' for regular n^2 self-attention, 'tvm' for TVM implemenation of Longformer\n",
    "                selfattention, 'sliding_chunks' for another implementation of Longformer selfattention\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention_window = attention_window\n",
    "        self.attention_dilation = attention_dilation\n",
    "        self.autoregressive = autoregressive\n",
    "        self.attention_mode = attention_mode\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        assert self.attention_mode in ['tvm', 'sliding_chunks', 'n2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_long_model(save_model_to, base_model, tokenizer_name_or_path, attention_window, max_pos):\n",
    "    tokenizer = MBart50Tokenizer.from_pretrained(tokenizer_name_or_path, model_max_length=max_pos)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(base_model)\n",
    "    config = LongformerEncoderDecoderConfig.from_pretrained(base_model)\n",
    "\n",
    "    model.config = config\n",
    "\n",
    "    # in BART attention_probs_dropout_prob is attention_dropout, but LongformerSelfAttention\n",
    "    # expects attention_probs_dropout_prob, so set it here\n",
    "\n",
    "    config.attention_probs_dropout_prob = config.attention_dropout\n",
    "    config.architectures = ['LongformerEncoderDecoderForConditionalGeneration', ]\n",
    "\n",
    "    # extend position embeddings\n",
    "    tokenizer.model_max_length = max_pos\n",
    "    tokenizer.init_kwargs['model_max_length'] = max_pos\n",
    "    current_max_pos, embed_size = model.model.encoder.embed_positions.weight.shape\n",
    "    assert current_max_pos == config.max_position_embeddings + 2\n",
    "\n",
    "    config.max_encoder_position_embeddings = max_pos\n",
    "    config.max_decoder_position_embeddings = config.max_position_embeddings\n",
    "    del config.max_position_embeddings\n",
    "    max_pos += 2  # NOTE: BART has positions 0,1 reserved, so embedding size is max position + 2\n",
    "    assert max_pos >= current_max_pos\n",
    "\n",
    "    # allocate a larger position embedding matrix for the encoder\n",
    "    new_encoder_pos_embed = model.model.encoder.embed_positions.weight.new_empty(max_pos, embed_size)\n",
    "\n",
    "\n",
    "    print(f'new_encoder_pos_embed: {new_encoder_pos_embed}')\n",
    "    print(f'new_encoder_pos_embed.shape: {new_encoder_pos_embed.shape}')\n",
    "\n",
    "    # copy position embeddings over and over to initialize the new position embeddings\n",
    "    k = 2\n",
    "    step = current_max_pos - 2\n",
    "    while k < max_pos - 1:\n",
    "        new_encoder_pos_embed[k:(\n",
    "            k + step)] = model.model.encoder.embed_positions.weight[2:]\n",
    "        k += step\n",
    "        \n",
    "    model.model.encoder.embed_positions.weight.data = new_encoder_pos_embed\n",
    "\n",
    "    # allocate a larger position embedding matrix for the decoder\n",
    "    # new_decoder_pos_embed = model.model.decoder.embed_positions.weight.new_empty(max_pos, embed_size)\n",
    "    # # copy position embeddings over and over to initialize the new position embeddings\n",
    "    # k = 2\n",
    "    # step = current_max_pos - 2\n",
    "    # while k < max_pos - 1:\n",
    "    #     new_decoder_pos_embed[k:(k + step)] = model.model.decoder.embed_positions.weight[2:]\n",
    "    #     k += step\n",
    "    # model.model.decoder.embed_positions.weight.data = new_decoder_pos_embed\n",
    "\n",
    "    # replace the `modeling_bart.SelfAttention` object with `LongformerSelfAttention`\n",
    "\n",
    "    config.attention_window = [attention_window] * config.num_hidden_layers\n",
    "    config.attention_dilation = [1] * config.num_hidden_layers\n",
    "\n",
    "    for i, layer in enumerate(model.model.encoder.layers):\n",
    "        longformer_self_attn_for_mbart = LongformerSelfAttentionForMBart(config, layer_id=i)\n",
    "\n",
    "        longformer_self_attn_for_mbart.longformer_self_attn.query = layer.self_attn.q_proj\n",
    "        longformer_self_attn_for_mbart.longformer_self_attn.key = layer.self_attn.k_proj\n",
    "        longformer_self_attn_for_mbart.longformer_self_attn.value = layer.self_attn.v_proj\n",
    "\n",
    "        longformer_self_attn_for_mbart.longformer_self_attn.query_global = copy.deepcopy(\n",
    "            layer.self_attn.q_proj)\n",
    "        longformer_self_attn_for_mbart.longformer_self_attn.key_global = copy.deepcopy(\n",
    "            layer.self_attn.k_proj)\n",
    "        longformer_self_attn_for_mbart.longformer_self_attn.value_global = copy.deepcopy(\n",
    "            layer.self_attn.v_proj)\n",
    "\n",
    "        longformer_self_attn_for_mbart.output = layer.self_attn.out_proj\n",
    "\n",
    "        layer.self_attn = longformer_self_attn_for_mbart\n",
    "\n",
    "    logger.info(f'saving model to {save_model_to}')\n",
    "    model.save_pretrained(save_model_to)\n",
    "    tokenizer.save_pretrained(save_model_to, None)\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model arguments: \n",
      "save_model_to=./tmp/20k/mbart-long, base_model=facebook/mbart-large-50, tokenizer_name_or_path=facebook/mbart-large-50, attention_window=512, max_pos=20480\n",
      "creating new model...\n",
      "new_encoder_pos_embed: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "new_encoder_pos_embed.shape: torch.Size([20482, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:saving model to ./tmp/20k/mbart-long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: MBartConfig {\n",
      "  \"_name_or_path\": \"./tmp/20k/mbart-long\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dilation\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_mode\": \"sliding_chunks\",\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"autoregressive\": false,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 20480,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "before if statement\n",
      "instantiating MBartLearnedPositionalEmbedding for encoder...\n",
      "instantiating MBartLearnedPositionalEmbedding for decoder...\n",
      "replacing attention with long attention...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "save_model_to = './tmp/20k/mbart-long'# './tmp/mbart-long'\n",
    "base_model = 'facebook/mbart-large-50'\n",
    "tokenizer_name_or_path = 'facebook/mbart-large-50'\n",
    "attention_window = 512\n",
    "max_pos =  20480 # 16384 # mutiple of origin maximum encoder positions ?\n",
    "\n",
    "print('new model arguments: ')\n",
    "print(f'save_model_to={save_model_to}, base_model={base_model}, tokenizer_name_or_path={tokenizer_name_or_path}, attention_window={attention_window}, max_pos={max_pos}')\n",
    "print('creating new model...')\n",
    "\n",
    "create_long_model(\n",
    "    save_model_to=save_model_to,\n",
    "    base_model=base_model,\n",
    "    tokenizer_name_or_path=tokenizer_name_or_path,\n",
    "    attention_window=attention_window,\n",
    "    max_pos=max_pos\n",
    ")\n",
    "\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(save_model_to)\n",
    "model = LongformerEncoderDecoderForConditionalGeneration.from_pretrained(save_model_to)\n",
    "\n",
    "model.model.encoder.config.gradient_checkpointing = True\n",
    "model.model.decoder.config.gradient_checkpointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongformerEncoderDecoderForConditionalGeneration(\n",
      "  (model): MBartModel(\n",
      "    (shared): Embedding(250054, 1024, padding_idx=1)\n",
      "    (encoder): MBartEncoder(\n",
      "      (embed_tokens): Embedding(250054, 1024, padding_idx=1)\n",
      "      (embed_positions): MBartLearnedPositionalEmbedding(20482, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): MBartEncoderLayer(\n",
      "          (self_attn): LongformerSelfAttentionForMBart(\n",
      "            (longformer_self_attn): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): MBartDecoder(\n",
      "      (embed_tokens): Embedding(250054, 1024, padding_idx=1)\n",
      "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  16384\n",
      "Rimsko cesarstvo (latinsko Imperivm Romanvm, grko   , Basilea tn Rhman) je bilo obdobje starega Rima, ki je sledilo Rimski republiki. Kot drava je obsegalo veliko ozemlje okoli Sredozemskega morja v Evropi, severni Afriki in zahodni Aziji. V cesarstvu so vladali cesarji. Od zaetka vladavine cesarja Avgusta do vojake anarhije v 3. stoletju je bila drava principat z Italijo kot metropolo provinc in Rimom kot edino prestolnico (27 pr. n. t. - 286 n. m.). Po krizi 3. stoletja je bilo cesarstvo razdeljeno v Zahodno rimske cesarstvo in Vzhodno rimsko cearstvo. Slednje je znano tudi kot Bizantinsko cesarstvo. Cesarstvi sta imeli vsako svojega cesarja. Uradna prestolnica obeh cesarstev je do leta 476 ostal Rim. Tisto leto so Raveno zasedli Odoakerjevi Ostrogoti in odstavili zadnjega zahodnorimskega cesarja Romula Avgust.\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 512 * 32\n",
    "\n",
    "def summarize(text, max_len):\n",
    "\n",
    "    context_tokens = ['<s>'] + tokenizer.tokenize(text) + ['</s>']\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(context_tokens) \n",
    "\n",
    "    if len(input_ids) < max_seq_len:   \n",
    "            while len(input_ids) < max_seq_len: \n",
    "                input_ids += [tokenizer.pad_token_id] \n",
    "    else:\n",
    "        input_ids = input_ids[:max_seq_len - 1] + [   \n",
    "            tokenizer.eos_token_id]\n",
    "\n",
    "    print('input_ids: ',len(input_ids))\n",
    "\n",
    "\n",
    "    model.model.encoder.config.gradient_checkpointing = True\n",
    "    model.model.decoder.config.gradient_checkpointing = True\n",
    "\n",
    "    res_ids = model.generate(torch.tensor([input_ids]),\n",
    "                                        max_length=max_len,\n",
    "                                        num_beams=5,\n",
    "                                        no_repeat_ngram_size = 3,\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        bad_words_ids=[[tokenizer.unk_token_id]])        \n",
    "    res = tokenizer.batch_decode(res_ids.tolist(), skip_special_tokens=True)[0]\n",
    "    \n",
    "    return print(res)\n",
    "\n",
    "summarize(\"Rimsko cesarstvo (latinsko Imperivm Romanvm, grko   , Basilea tn Rhman) je bilo obdobje starega Rima, ki je sledilo Rimski republiki. Kot drava je obsegalo veliko ozemlje okoli Sredozemskega morja v Evropi, severni Afriki in zahodni Aziji. V cesarstvu so vladali cesarji. Od zaetka vladavine cesarja Avgusta do vojake anarhije v 3. stoletju je bila drava principat z Italijo kot metropolo provinc in Rimom kot edino prestolnico (27 pr. n. t. - 286 n. t.). Po krizi 3. stoletja je bilo cesarstvo razdeljeno v Zahodno rimsko cesarstvo in Vzhodno rimsko cesarstvo. Slednje je znano tudi kot Bizantinsko cesarstvo. Cesarstvi sta imeli vsako svojega cesarja. Uradna prestolnica obeh cesarstev je do leta 476 ostal Rim. Tisto leto so Raveno zasedli Odoakerjevi Ostrogoti in odstavili zadnjega zahodnorimskega cesarja Romula Avgusta, zato so cesarske insignije prenesli v Konstantinopel. S sprejetjem kranstva kot dravne vere Rimskega cesarstva leta 380 in padcem Zahodnega rimskega cesarstva se je konalo obdobje klasine antike in zael srednji vek. Ti dogodki in postopna helenizacija Vzhodnega rimskega cesarstva so razlog, da zgodovinarji srednjeveko Rimsko cesarstvo, ki je ostalo v vzhodnih rimskih provincah, imenujejo Bizantinsko cesarstvo. Prvi dve stoletji cesarstva sta bili obdobje stabilnosti in razcveta brez primere, znano kot Pax Romana (rimski mir). V 3. stoletju je cesarstvo doivelo krizo, ki je ogrozila njegov obstoj, saj sta se Galsko in Palmirsko cesarstvo odcepila. Na prestolu se je zvrstila vrsta kratkoivih cesarjev, pogosto legionarjev in pogosto ve hkrati. Cesarstvo je ponovno zdruil Avrelijan (270275). Dioklecijan je poskusil cesarstvo stabilizirati in ga je leta 286 razdelil na latinski zahod in grki vzhod. V 4. stoletju so se po Milanskem ediktu iz leta 313 zaeli na vplivne dravne poloaje vzpenjati kristjani. Kmalu zatem se je zaelo obdobje velikih selitev, obsenih vpadov germanskih ljudstev in Hunov pod vodstvom Atile, kar je pripeljalo do propada Zahodnega rimskega cesarstva. S padcem Ravene pod germanske Herule in z Odoakerjevo odstavitvijo cesarja Romula Avgusta leta 476 je Zahodno rimsko cesarstvo dokonno propadlo, kar je formalno potrdil vzhodnorimski cesar Zenon leta 480. Nekatere drave na ozemljih nekdanjega Zahodnega rimskega cesarstva so se kasneje imele za njegove dedie in so se potegovale za vrhovno oblast rimskih cesarjev. Najpomembneje med njimi je bilo Sveto rimsko cesarstvo. Vzhodno rimsko cesarstvo je preivelo e celo tisoletje, dokler niso Konstantinopla leta 1453 zavzeli osmanski Turki pod vodstvom sultana Mehmeda II.[op 4] Zaradi velikega ozemlja in dolgega obstoja Rimskega cesarstva so rimske upravne prakse in kultura mono in trajno vplivale na razvoj jezika, religije, umetnosti, arhitekture, filozofije, prava in oblik vladanja ne samo na ozemlju, ki ga je obsegalo, temve tudi dale preko meja. Jezik Rimljanov se je razvil v romanske jezike srednjevekega in modernega sveta, medtem ko je klasina grina postala jezik Vzhodnega rimskega cesarstva. Sprejetje kranstva v cesarstvu je privedlo do oblikovanja srednjevekega kranstva. Grka in rimska umetnost sta mono vplivali na italijansko renesanso. Rimska arhitekturna tradicija je sluila kot osnova za romansko, renesanno in neoklasino arhitekturo, mono pa je vplivala tudi na islamsko arhitekturo. Rimsko pravo ima naslednike v mnogih dananjih pravnih sistemih, na primer v Napoleonovem zakoniku, pa tudi v francoskem in italijanskem sodobnem pravu; rimske republike institucije so pustile trajno zapuino, ki je vplivala na srednjeveke italijanske mestne republike in drave, pozneje pa tudi na politino ureditev v ZDA in drugih modernih \", 512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('diploma': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7dd6fc77cfe7929ab6e14567fa546fdaf6d03841859679a01b012c77d7e8b345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
